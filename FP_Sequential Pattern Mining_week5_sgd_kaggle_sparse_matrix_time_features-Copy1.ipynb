{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/data) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle_data/train_sessions.csv', index_col='session_id')\n",
    "test_df = pd.read_csv('kaggle_data/test_sessions.csv', index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>932.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>151.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3191.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>2014-02-14 15:16:45</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:17:13</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2014-02-14 15:20:47</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:21:13</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1943</td>\n",
       "      <td>2014-03-17 15:19:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:20:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:21:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:42</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:43</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2   site3  \\\n",
       "session_id                                                                    \n",
       "1               8  2014-01-04 08:44:50    11.0  2014-01-04 08:44:50    82.0   \n",
       "2             111  2014-03-18 10:33:20    78.0  2014-03-18 10:33:31   151.0   \n",
       "3              11  2014-12-02 13:13:41  3187.0  2014-12-02 13:13:41   132.0   \n",
       "4             668  2014-02-14 15:16:45  1965.0  2014-02-14 15:17:13   598.0   \n",
       "5            1943  2014-03-17 15:19:40  1943.0  2014-03-17 15:20:10  1943.0   \n",
       "\n",
       "                          time3   site4                time4   site5  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:19    68.0  2014-01-04 08:45:25     8.0   \n",
       "2           2014-03-18 10:33:31   111.0  2014-03-18 10:33:31  1401.0   \n",
       "3           2014-12-02 13:13:42   496.0  2014-12-02 13:13:42  1969.0   \n",
       "4           2014-02-14 15:20:47  1965.0  2014-02-14 15:21:13   284.0   \n",
       "5           2014-03-17 15:21:40  1943.0  2014-03-17 15:22:10  1943.0   \n",
       "\n",
       "                          time5   ...                  time6   site7  \\\n",
       "session_id                        ...                                  \n",
       "1           2014-01-04 08:45:25   ...    2014-01-04 08:45:51  8403.0   \n",
       "2           2014-03-18 10:33:31   ...    2014-03-18 10:33:32  1375.0   \n",
       "3           2014-12-02 13:13:45   ...    2014-12-02 13:13:45  3187.0   \n",
       "4           2014-02-14 15:21:14   ...    2014-02-14 15:21:14    38.0   \n",
       "5           2014-03-17 15:22:39   ...    2014-03-17 15:22:39  1952.0   \n",
       "\n",
       "                          time7   site8                time8   site9  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:51   932.0  2014-01-04 08:45:53  3260.0   \n",
       "2           2014-03-18 10:33:32    38.0  2014-03-18 10:33:32  1401.0   \n",
       "3           2014-12-02 13:13:45    82.0  2014-12-02 13:13:46  3191.0   \n",
       "4           2014-02-14 15:21:14  4451.0  2014-02-14 15:21:14  4537.0   \n",
       "5           2014-03-17 15:22:41  1943.0  2014-03-17 15:22:41  1943.0   \n",
       "\n",
       "                          time9  site10               time10 user_id  \n",
       "session_id                                                            \n",
       "1           2014-01-04 08:45:53     8.0  2014-01-04 08:45:53    1845  \n",
       "2           2014-03-18 10:33:32    97.0  2014-03-18 10:33:34    3322  \n",
       "3           2014-12-02 13:13:46  3184.0  2014-12-02 13:13:47    2003  \n",
       "4           2014-02-14 15:21:15    11.0  2014-02-14 15:21:15    1373  \n",
       "5           2014-03-17 15:22:42  1943.0  2014-03-17 15:22:43    1737  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 21), (41177, 20), (136496, 21))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 550 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза ID пользователя будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sites = train_df[['site1', 'site2', 'site3','site4','site5','site6','site7',\n",
    "          'site8', 'site9', 'site10', 'user_id']].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ures_top_sites(data, threshold=1):\n",
    "    \"\"\" input dataframe\n",
    "    return \n",
    "    list of top sites for users with the threshold\n",
    "    dictionary user, top site's list \"\"\"\n",
    "    top_users_sites =[]\n",
    "    dic_user_top ={}\n",
    "    \n",
    "    for user, values in pd.groupby(data, by = 'user_id'):\n",
    "        n,m = values.shape\n",
    "        sites, freq = np.unique(np.ravel(values.values), return_counts=True)\n",
    "        mask = np.logical_not(np.logical_not(sites))\n",
    "        sites = sites[mask]\n",
    "        freq = freq[mask]\n",
    "        sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "        #user_site_top = sort_sites[:threshold]\n",
    "        top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "        dic_user_top[user] = top_list\n",
    "        top_users_sites.append(top_list)\n",
    "    return np.unique(top_users_sites), dic_user_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tops_sites(data, threshold=3):\n",
    "    \"\"\" input dataframe\n",
    "    return top sites with the frequensy by  threshold\"\"\"\n",
    "    data_ravel =np.ravel(data.drop(['user_id'], axis =1).values)\n",
    "    sites, freq = np.unique(data_ravel, return_counts=True)\n",
    "    mask = np.logical_not(np.logical_not(sites))\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    mask= freq >= threshold\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "    #user_site_top = sort_sites[:threshold]\n",
    "    top_list = np.array([x[0] for x in  sort_sites])\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_sites(data, threshold=50):\n",
    "    \"\"\" input dataframe\n",
    "    return top sites with the threshold\"\"\"\n",
    "    data_ravel =np.ravel(data.drop(['user_id'], axis =1).values)\n",
    "    sites, freq = np.unique(data_ravel, return_counts=True)\n",
    "    mask = np.logical_not(np.logical_not(sites))\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "    #user_site_top = sort_sites[:threshold]\n",
    "    top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из 50 самых популярных сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_sites = top_sites(train_sites, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, array([  32,    8, 1943,   22,   11,   38, 1945,  237,   12, 1940,  251,\n",
       "           9, 1942,   63,   55,   82,   67,   97,    7,   88,  419,  307,\n",
       "          69,  111,  280,   77,  106,  151,  298,   65,   70,   27,   71,\n",
       "          64,  523,  690,  521,  422, 9027,    1,  662, 2184,   78,  305,\n",
       "          85,   14,  526,  255,  243,    6]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_sites), top_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из сайтов которые повторяются минимум 2 раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sites = tops_sites(train_sites, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13988, array([  32,    8, 1943,   22,   11,   38, 1945,  237,   12, 1940]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sites), list_sites[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_sites = {x: np.int32(i+1) for i,x in enumerate(list_sites) } # словарь для переименования сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in2D(data, ar2):\n",
    "    \"\"\" return the mask like data where True only the element of data is in ar2\"\"\"\n",
    "    data = data.values\n",
    "    n,m = data.shape\n",
    "    mask = np.in1d(np.ravel(data),ar2)\n",
    "    mask_2d = mask.reshape((n,m))\n",
    "    return mask_2d\n",
    "#np.where(mask_2d,train_sites.head(3).values, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создадим признаки связанные с временем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>time4</th>\n",
       "      <th>time5</th>\n",
       "      <th>time6</th>\n",
       "      <th>time7</th>\n",
       "      <th>time8</th>\n",
       "      <th>time9</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time1                time2                time3  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:44:50  2014-01-04 08:44:50  2014-01-04 08:45:19   \n",
       "2           2014-03-18 10:33:20  2014-03-18 10:33:31  2014-03-18 10:33:31   \n",
       "3           2014-12-02 13:13:41  2014-12-02 13:13:41  2014-12-02 13:13:42   \n",
       "\n",
       "                          time4                time5                time6  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:25  2014-01-04 08:45:25  2014-01-04 08:45:51   \n",
       "2           2014-03-18 10:33:31  2014-03-18 10:33:31  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:42  2014-12-02 13:13:45  2014-12-02 13:13:45   \n",
       "\n",
       "                          time7                time8                time9  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:51  2014-01-04 08:45:53  2014-01-04 08:45:53   \n",
       "2           2014-03-18 10:33:32  2014-03-18 10:33:32  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:45  2014-12-02 13:13:46  2014-12-02 13:13:46   \n",
       "\n",
       "                         time10  \n",
       "session_id                       \n",
       "1           2014-01-04 08:45:53  \n",
       "2           2014-03-18 10:33:34  \n",
       "3           2014-12-02 13:13:47  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_time = train_test_df[['time1', 'time2', 'time3', 'time4','time5', 'time6',\n",
    "                        'time7','time8', 'time9', 'time10']].fillna(np.datetime64('nat'))\n",
    "train_test_time.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                                     'site6','site7', 'site8', 'site9', 'site10']].fillna(np.int32(0)).astype(np.int32)\n",
    "y = train_df['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask1 = in2D(train_test_df_sites, top_sites) # маска для сайтов самых популярных у пользователей\n",
    "mask2 = in2D(train_test_df_sites, list_sites)  # маска для сайтов которые встречаются в выборке больше 2 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**выделим сайты которые идут за сымыми популярными**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_sites[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только сайты с частотой более 2\n",
    "train_test_df_sites = np.where(mask2, train_test_df_sites.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496L, 10L)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496L, 10L)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.vectorize(lambda x: dic_sites[x])\n",
    "train_test_df_replace = f(train_test_df_sites)\n",
    "train_test_df_replace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites_shift = train_test_df_sites[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496L, 9L), (136496L, 9L))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_top_site = np.where(mask1[:,:-1], train_test_df_sites_shift, 0) # site after top50\n",
    "next_top_site.shape,  train_test_df_sites_shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_time.apply(pd.to_datetime).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только сайты с частотой более 2\n",
    "train_test_df_time = np.where(mask2, train_test_df_time, train_test_df_time.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# начало сессии по дням недели\n",
    "day_of_week = (pd.to_datetime(np.min(train_test_df_time, axis =1))).dayofweek\n",
    "day_of_week = day_of_week.reshape(train_test_df_time.shape[0],1).astype(int) \n",
    "# время начала сессии в часах\n",
    "start_hour = (pd.to_datetime(np.min(train_test_df_time, axis =1))).hour\n",
    "start_hour = start_hour.reshape(train_test_df_time.shape[0],1).astype(int)\n",
    "# время захода на сайт из top50  \n",
    "time_tops_site = np.where(mask1, train_test_df_time, train_test_df_time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_time = np.where(mask2, train_test_df_time, np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время перехода между сайтами в 3 сек \n",
    "train_test_time_diff = (np.diff(train_test_df_time, axis =1)/np.timedelta64(2, 's')).round()\n",
    "#test_time_diff = (np.diff(test_time, axis =1)/np.timedelta64(3, 's')).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время перехода между сайтами из top50 and next в 2 сек \n",
    "next_time_diff_tops = np.where(mask1[:,:-1], train_test_time_diff, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_time_diff_tops.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время захода на сайт из top50 в часах\n",
    "top_start_hour = (pd.to_datetime(np.min(time_tops_site, axis =1))).hour \n",
    "top_start_hour = top_start_hour.reshape(time_tops_site.shape[0],1).astype(int) +1\n",
    "top_start_hour = np.nan_to_num(top_start_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**перевод категориальных признаков в двоичные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 7), (136496, 17), (136496, 17))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "day_of_week_encode = OneHotEncoder().fit_transform(day_of_week) # перевод категориальных признаков в двоичные\n",
    "start_hour_encode = OneHotEncoder().fit_transform(start_hour)\n",
    "top_start_hour_encode = OneHotEncoder().fit_transform(top_start_hour)\n",
    "day_of_week_encode.shape, start_hour_encode.shape, top_start_hour_encode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создаем разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как делали ранее. Используем объединенную матрицу train_test_df_sites – потом разделим обратно на обучающую и тестовую части.**\n",
    "\n",
    "\n",
    "**Выделим в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_sparse_matrix (matrix):\n",
    "    \"\"\"переводим обычную матрицу в разреженноу матрицу \n",
    "    где \n",
    "    номер столбца это уникальное число из исходной матрицы от 1  до максимального\n",
    "    значение в строке это сколько раз уникальное число встречалось в строке оригинальной матрицы\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "        \n",
    "    NMZ = np.prod(np.array(matrix.shape)) # колличество элементов в matrix\n",
    "    data = np.array([1]*NMZ)\n",
    "    indptr = np.arange(0, NMZ+matrix.shape[1], matrix.shape[1])\n",
    "    return csr_matrix((data, matrix.reshape(-1), indptr), dtype=int)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 21532)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_test_sparse = matrix_to_sparse_matrix(train_test_df_sites.astype(np.int64))\n",
    "Train_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 21532)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Next_top_site_sparse = matrix_to_sparse_matrix(next_top_site.astype(np.int64))\n",
    "Next_top_site_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 900)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Next_time_tops_sparse =  matrix_to_sparse_matrix(next_time_diff_tops)\n",
    "Next_time_tops_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**объединим все признаки сайты время начала сесии время захода на сайт из top200 and top5user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour\n",
    "Train_Test_1 = hstack((Train_test_sparse, day_of_week_encode, start_hour_encode, top_start_hour_encode ))\n",
    "Train_Test_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**разделим признаки на Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_spare = Train_Test_1.tocsr()[: 95319, :]\n",
    "#X_test_sparse = Train_Test_1.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_test_sparse.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_test_sparse.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создаем объекты sklearn.linear_model.SGDClassifier с логистической функцией потерь и с hinge loss (логистическая регрессия и линейный SVM соответственно) . Остальные параметры  по умолчанию, разве что alpha=0.00007, n_jobs=-1 никогда не помешает. Обучите  модели на выборке (X_train, y_train).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим и проверим на валидации два классификатора SGDClassifier( alpha=0.00007, loss ='log') и SGDClassifier( alpha=0.00007, loss ='hinge')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_SGD (Train, y, explain = \"fit_SGD\"):\n",
    "    # Разобьем обучающую выборку на 2 части в пропорции 7/3\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size=0.3, \n",
    "                                                     random_state=0, stratify=y)\n",
    "    \n",
    "    sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 0,  n_jobs = -1)\n",
    "    sgd_svm  = SGDClassifier( alpha=0.00007, loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "    print('fit...')\n",
    "    %time sgd_logit.fit(X_train, y_train)\n",
    "    %time sgd_svm.fit(X_train, y_train)\n",
    "    # Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\n",
    "    print('predict...')\n",
    "    pred_log = sgd_logit.predict(X_valid)\n",
    "    pred_svm = sgd_svm.predict(X_valid)\n",
    "    print (explain)\n",
    "    print('log',accuracy_score(y_valid, pred_log))\n",
    "    print('svm',accuracy_score(y_valid, pred_svm))\n",
    "    return sgd_logit, sgd_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 37.1 s\n",
      "Wall time: 26.1 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour\n",
      "('log', 0.36116939432088402)\n",
      "('svm', 0.33662050636452651)\n"
     ]
    }
   ],
   "source": [
    "log_pred1, svm_pred1 = fit_SGD (X_train_spare, y, explain = \"sites + day_of_week+ start_hour + top50 hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 43105)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour + next after top50 site\n",
    "Train_Test_2 = hstack((Train_Test_1, Next_top_site_sparse))\n",
    "Train_Test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_2.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_2.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 41.3 s\n",
      "Wall time: 31.3 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour + next after top50 site\n",
      "('log', 0.35763743180864455)\n",
      "('svm', 0.33004616030214018)\n"
     ]
    }
   ],
   "source": [
    "log2, svm2 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour + next after top50 site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 22473)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour + time of next top50 site\n",
    "Train_Test_3 = hstack((Train_Test_1, Next_time_tops_sparse))\n",
    "Train_Test_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_3.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_3.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 43.2 s\n",
      "Wall time: 33.2 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour + time of next top50 site\n",
      "('log', 0.36151909357952161)\n",
      "('svm', 0.33648062666107148)\n"
     ]
    }
   ],
   "source": [
    "log3, svm3 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour + time of next top50 site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 44005)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\n",
    "Train_Test_4 = hstack((Train_Test_2, Next_time_tops_sparse))\n",
    "Train_Test_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_4.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_4.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 43.8 s\n",
      "Wall time: 32.9 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\n",
      "('log', 0.35616869492236675)\n",
      "('svm', 0.32955658134004756)\n"
     ]
    }
   ],
   "source": [
    "log4, svm4 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз для тестовой выборки с помощью log3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = log3.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file (pred_test, 'kaggle_data/[YDF&MIPT]_Coursera_Oleg67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
