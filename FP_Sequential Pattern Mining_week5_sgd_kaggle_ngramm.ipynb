{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "\n",
    "вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/data) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle_data/train_sessions.csv', index_col='session_id')\n",
    "test_df = pd.read_csv('kaggle_data/test_sessions.csv', index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>932.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>151.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3191.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>2014-02-14 15:16:45</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:17:13</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2014-02-14 15:20:47</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:21:13</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1943</td>\n",
       "      <td>2014-03-17 15:19:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:20:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:21:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:42</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:43</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2   site3  \\\n",
       "session_id                                                                    \n",
       "1               8  2014-01-04 08:44:50    11.0  2014-01-04 08:44:50    82.0   \n",
       "2             111  2014-03-18 10:33:20    78.0  2014-03-18 10:33:31   151.0   \n",
       "3              11  2014-12-02 13:13:41  3187.0  2014-12-02 13:13:41   132.0   \n",
       "4             668  2014-02-14 15:16:45  1965.0  2014-02-14 15:17:13   598.0   \n",
       "5            1943  2014-03-17 15:19:40  1943.0  2014-03-17 15:20:10  1943.0   \n",
       "\n",
       "                          time3   site4                time4   site5  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:19    68.0  2014-01-04 08:45:25     8.0   \n",
       "2           2014-03-18 10:33:31   111.0  2014-03-18 10:33:31  1401.0   \n",
       "3           2014-12-02 13:13:42   496.0  2014-12-02 13:13:42  1969.0   \n",
       "4           2014-02-14 15:20:47  1965.0  2014-02-14 15:21:13   284.0   \n",
       "5           2014-03-17 15:21:40  1943.0  2014-03-17 15:22:10  1943.0   \n",
       "\n",
       "                          time5   ...                  time6   site7  \\\n",
       "session_id                        ...                                  \n",
       "1           2014-01-04 08:45:25   ...    2014-01-04 08:45:51  8403.0   \n",
       "2           2014-03-18 10:33:31   ...    2014-03-18 10:33:32  1375.0   \n",
       "3           2014-12-02 13:13:45   ...    2014-12-02 13:13:45  3187.0   \n",
       "4           2014-02-14 15:21:14   ...    2014-02-14 15:21:14    38.0   \n",
       "5           2014-03-17 15:22:39   ...    2014-03-17 15:22:39  1952.0   \n",
       "\n",
       "                          time7   site8                time8   site9  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:51   932.0  2014-01-04 08:45:53  3260.0   \n",
       "2           2014-03-18 10:33:32    38.0  2014-03-18 10:33:32  1401.0   \n",
       "3           2014-12-02 13:13:45    82.0  2014-12-02 13:13:46  3191.0   \n",
       "4           2014-02-14 15:21:14  4451.0  2014-02-14 15:21:14  4537.0   \n",
       "5           2014-03-17 15:22:41  1943.0  2014-03-17 15:22:41  1943.0   \n",
       "\n",
       "                          time9  site10               time10 user_id  \n",
       "session_id                                                            \n",
       "1           2014-01-04 08:45:53     8.0  2014-01-04 08:45:53    1845  \n",
       "2           2014-03-18 10:33:32    97.0  2014-03-18 10:33:34    3322  \n",
       "3           2014-12-02 13:13:46  3184.0  2014-12-02 13:13:47    2003  \n",
       "4           2014-02-14 15:21:15    11.0  2014-02-14 15:21:15    1373  \n",
       "5           2014-03-17 15:22:42  1943.0  2014-03-17 15:22:43    1737  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 21), (41177, 20))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 550 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза ID пользователя будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sites = train_df[['site1', 'site2', 'site3','site4','site5','site6','site7',\n",
    "          'site8', 'site9', 'site10', 'user_id']].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ures_top_sites(data, threshold=1):\n",
    "    \"\"\" input dataframe\n",
    "    return \n",
    "    list of top sites for users with the threshold\n",
    "    dictionary user, top site's list \"\"\"\n",
    "    top_users_sites =[]\n",
    "    dic_user_top ={}\n",
    "    \n",
    "    for user, values in pd.groupby(data, by = 'user_id'):\n",
    "        n,m = values.shape\n",
    "        sites, freq = np.unique(np.ravel(values.values), return_counts=True)\n",
    "        mask = np.logical_not(np.logical_not(sites))\n",
    "        sites = sites[mask]\n",
    "        freq = freq[mask]\n",
    "        sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "        #user_site_top = sort_sites[:threshold]\n",
    "        top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "        dic_user_top[user] = top_list\n",
    "        top_users_sites.append(top_list)\n",
    "    return np.unique(top_users_sites), dic_user_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sites(data, threshold=30):\n",
    "    \"\"\" input dataframe\n",
    "    return top sites with the threshold\"\"\"\n",
    "    data_ravel =np.ravel(data.drop(['user_id'], axis =1).values)\n",
    "    sites, freq = np.unique(data_ravel, return_counts=True)\n",
    "    mask = np.logical_not(np.logical_not(sites))\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "    #user_site_top = sort_sites[:threshold]\n",
    "    top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из 10 самых популярных для каждого пользователя сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_top_sites, dic_user_sites = ures_top_sites(train_sites, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_top_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из 300 самых популярных  сайтов в выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list_sites = top_sites(train_sites, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in2D(data, ar2):\n",
    "    \"\"\" return the mask like data where True only the element of data is in ar2\"\"\"\n",
    "    data = data.values\n",
    "    n,m = data.shape\n",
    "    mask = np.in1d(np.ravel(data),ar2)\n",
    "    mask_2d = mask.reshape((n,m))\n",
    "    return mask_2d\n",
    "#np.where(mask_2d,train_sites.head(3).values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gramm_2 (data):\n",
    "    \"\"\" return 2_gramm from all rows of data \"\"\"\n",
    "    data = data.values\n",
    "    n,m = data.shape\n",
    "    ngramm = np.empty((n,m-1), dtype = object)\n",
    "    for  i, list_ in enumerate (data.tolist()):\n",
    "        ngramm[i] =[str(x[0])+'andsite'+str(x[1]) for x in zip(list_[:-1], list_[1:])]\n",
    "    return ngramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создадим признаки связанные с временем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>time4</th>\n",
       "      <th>time5</th>\n",
       "      <th>time6</th>\n",
       "      <th>time7</th>\n",
       "      <th>time8</th>\n",
       "      <th>time9</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time1                time2                time3  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:44:50  2014-01-04 08:44:50  2014-01-04 08:45:19   \n",
       "2           2014-03-18 10:33:20  2014-03-18 10:33:31  2014-03-18 10:33:31   \n",
       "3           2014-12-02 13:13:41  2014-12-02 13:13:41  2014-12-02 13:13:42   \n",
       "\n",
       "                          time4                time5                time6  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:25  2014-01-04 08:45:25  2014-01-04 08:45:51   \n",
       "2           2014-03-18 10:33:31  2014-03-18 10:33:31  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:42  2014-12-02 13:13:45  2014-12-02 13:13:45   \n",
       "\n",
       "                          time7                time8                time9  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:51  2014-01-04 08:45:53  2014-01-04 08:45:53   \n",
       "2           2014-03-18 10:33:32  2014-03-18 10:33:32  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:45  2014-12-02 13:13:46  2014-12-02 13:13:46   \n",
       "\n",
       "                         time10  \n",
       "session_id                       \n",
       "1           2014-01-04 08:45:53  \n",
       "2           2014-03-18 10:33:34  \n",
       "3           2014-12-02 13:13:47  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time = train_df[['time1', 'time2', 'time3', 'time4','time5', 'time6',\n",
    "                        'time7','time8', 'time9', 'time10']].fillna(np.datetime64('nat'))\n",
    "train_time.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sites = test_df[['site1', 'site2', 'site3','site4','site5','site6','site7',\n",
    "                      'site8', 'site9', 'site10']].fillna(0).astype('int')\n",
    "test_time = test_df[['time1', 'time2', 'time3', 'time4','time5', 'time6',\n",
    "                        'time7','time8', 'time9', 'time10']].fillna(np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим 2 граммы сайтов идущих подряд в каждой сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2gramm = gramm_2 (train_sites.drop(['user_id'], axis =1))\n",
    "X_test_2gramm = gramm_2 (test_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- переведем строчки базы sites в строчки типа документов, сначала заменим 0 на пробел чтобы избежать обучения на 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = train_sites.drop(['user_id'], axis =1).astype(str).replace('0','')\n",
    "test_str = test_sites.astype(str).replace('0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = train_str.apply(lambda x: ' '.join(x), axis =1)\n",
    "test_str = test_str.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = train_time.apply(pd.to_datetime).values\n",
    "test_time = test_time.apply(pd.to_datetime).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319L, 10L), (41177L, 10L))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time.shape, test_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# время перехода между сайтами в 3 сек \n",
    "train_time_diff = (np.diff(train_time, axis =1)/np.timedelta64(3, 's')).round()\n",
    "test_time_diff = (np.diff(test_time, axis =1)/np.timedelta64(3, 's')).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_diff = np.nan_to_num(train_time_diff).astype(int) # заменим пропуски на ноль\n",
    "test_time_diff = np.nan_to_num(test_time_diff).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- к созданым 2gramm из посещенных сайтов в сессии добавим к этим 2gramm время перехода между сайтами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2gramm_time = X_train_2gramm +'andtime' + train_time_diff.astype(str)\n",
    "X_test_2gramm_time = X_test_2gramm +'andtime' + test_time_diff.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#cvt = CountVectorizer( ngram_range = (1,2), min_df =2)\n",
    "#tvt = TfidfVectorizer( ngram_range = (1,2), min_df =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngramm_analys (Train, Test, ngramm = (1,1) , min_freq = 2, analyzer = u'word'):\n",
    "    cvt = CountVectorizer( ngram_range = ngramm, min_df =min_freq, analyzer = analyzer)\n",
    "    tvt = TfidfVectorizer( ngram_range = ngramm, min_df =min_freq, analyzer = analyzer)\n",
    "    X_train_cvt = cvt.fit_transform(Train)\n",
    "    X_train_tvt = tvt.fit_transform(Train)\n",
    "    X_test_cvt  = cvt.transform(Test)\n",
    "    X_test_tvt  = tvt.transform(Test)\n",
    "    return X_train_cvt, X_train_tvt, X_test_cvt, X_test_tvt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**создадим признаки из списка посещенных сайтов ипользуя инструмент CountVectorizer, TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 60252), (95319, 60252))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt1, X_train_tvt1, X_test_cvt1, X_test_tvt1 = ngramm_analys (train_str,  test_str, ngramm =(1,2), min_freq =2)\n",
    "X_train_cvt1.shape, X_train_tvt1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим и проверим на валидации два классификатора SGDClassifier( alpha=0.00007, loss ='log') и SGDClassifier( alpha=0.00007, loss ='hinge')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_SGD (Train, y, explain = \"fit_SGD\"):\n",
    "    # Разобьем обучающую выборку на 2 части в пропорции 7/3\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size=0.3, \n",
    "                                                     random_state=5, stratify=y)\n",
    "    \n",
    "    sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 0,  n_jobs = -1)\n",
    "    sgd_svm  = SGDClassifier( alpha=0.00007, loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "    print('fit...')\n",
    "    %time sgd_logit.fit(X_train, y_train)\n",
    "    %time sgd_svm.fit(X_train, y_train)\n",
    "    # Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\n",
    "    print('predict...')\n",
    "    pred_log = sgd_logit.predict(X_valid)\n",
    "    pred_svm = sgd_svm.predict(X_valid)\n",
    "    print (explain)\n",
    "    accu = accuracy_score(y_valid, pred_log)\n",
    "    print('log',accuracy_score(y_valid, pred_log))\n",
    "    print('svm',accuracy_score(y_valid, pred_svm))\n",
    "    return sgd_logit, sgd_svm, accu*sgd_logit.predict_proba(X_valid), sgd_svm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 40.4 s\n",
      "Wall time: 29.5 s\n",
      "predict...\n",
      "only sites in session with CountVectorizer preperation\n",
      "('log', 0.29248845992446498)\n",
      "('svm', 0.27402433906840118)\n"
     ]
    }
   ],
   "source": [
    "sgd_log1, sgd_svm1, pred1, pred_svm1 = fit_SGD(X_train_cvt1,y,'only sites in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 40.8 s\n",
      "Wall time: 34.2 s\n",
      "predict...\n",
      "only sites in session with TfidfVectorizer preperation\n",
      "('log', 0.19775493075954678)\n",
      "('svm', 0.30791019723038188)\n"
     ]
    }
   ],
   "source": [
    "sgd_log2, sgd_svm2, pred2, pred_svm2 = fit_SGD(X_train_tvt1,y,'only sites in session with TfidfVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pred1 + pred2\n",
    "svm_pred = np.hstack((pred_svm1, pred_svm2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим 2gramm в качестве признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2gramm_str = pd.DataFrame(X_train_2gramm).apply(lambda x: ' '.join(x), axis =1)\n",
    "test_2gramm_str  = pd.DataFrame(X_test_2gramm).apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 39543), (95319, 39543))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt2, X_train_tvt2, X_test_cvt2, X_test_tvt2 = ngramm_analys (train_2gramm_str,test_2gramm_str,\\\n",
    "                                                                      ngramm =(1,2), min_freq =4)\n",
    "X_train_cvt2.shape, X_train_tvt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 44.9 s\n",
      "Wall time: 33.3 s\n",
      "predict...\n",
      "2gramm in session with CountVectorizer preperation\n",
      "('log', 0.21499510421037907)\n",
      "('svm', 0.20747657014967127)\n"
     ]
    }
   ],
   "source": [
    "sgd_log3, sgd_svm3, pred3, pred_svm3= fit_SGD(X_train_cvt2,y,'2gramm in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 40.8 s\n",
      "Wall time: 30.3 s\n",
      "predict...\n",
      "2gramm in session with TfidfVectorizer preperation\n",
      "('log', 0.13159183102531824)\n",
      "('svm', 0.23206042803189258)\n"
     ]
    }
   ],
   "source": [
    "sgd_log4, sgd_svm4, pred4, pred_svm4 = fit_SGD(X_train_tvt2,y,'2gramm in session with TfidfVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pred + pred3 + pred4\n",
    "svm_pred = np.hstack((svm_pred, pred_svm3, pred_svm4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим 2gramm + time diff в качестве признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_2gramm_time_str = pd.DataFrame(X_train_2gramm_time).apply(lambda x: ' '.join(x), axis =1)\n",
    "test_2gramm_time_str  = pd.DataFrame(X_test_2gramm_time).apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 33664), (95319, 33664))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt3, X_train_tvt3, X_test_cvt3, X_test_tvt3 =\\\n",
    "                        ngramm_analys (train_2gramm_time_str,test_2gramm_time_str, min_freq =3)\n",
    "X_train_cvt3.shape, X_train_tvt3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 36.7 s\n",
      "Wall time: 27.8 s\n",
      "predict...\n",
      "2gramm_time in session with CountVectorizer preperation\n",
      "('log', 0.19838438942509443)\n",
      "('svm', 0.20988949503427054)\n"
     ]
    }
   ],
   "source": [
    "sgd_log5, sgd_svm5, pred5, pred_svm5  = fit_SGD(X_train_cvt3,y,'2gramm_time in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 46.3 s\n",
      "Wall time: 28.3 s\n",
      "predict...\n",
      "2gramm_time in session with TfidfVectorizer preperation\n",
      "('log', 0.11760386067981536)\n",
      "('svm', 0.22583578122814379)\n"
     ]
    }
   ],
   "source": [
    "sgd_log6, sgd_svm6, pred6, pred_svm6 = fit_SGD(X_train_tvt3,y,'2gramm_time in session with TfidfVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pred + pred5 + pred6\n",
    "svm_pred = np.hstack((svm_pred, pred_svm5, pred_svm6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_step (X_train, X_test, clf1):\n",
    "    \n",
    "    n1,_ = X_train.shape\n",
    "    tr_pred1= clf1.predict(X_train).reshape(n1,1)\n",
    "    \n",
    "    \n",
    "    n2,_ = X_test.shape\n",
    "    ts_pred1= clf1.predict(X_test).reshape(n2,1)\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    OHEn = OneHotEncoder()\n",
    "    tr_encode1 = OHEn.fit_transform(tr_pred1)\n",
    "    ts_encode1 = OHEn.transform(ts_pred1)\n",
    "    \n",
    "    \n",
    "    return tr_encode1, ts_encode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_ts = next_step (X_train_tvt1, X_test_tvt1, sgd_svm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr2, X_ts2 = next_step (X_train_tvt2, X_test_tvt2, sgd_svm4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr3, X_ts3 = next_step (X_train_tvt3, X_test_tvt3, sgd_svm6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1641), (41177, 1641))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new = hstack((X_tr, X_tr2, X_tr3))\n",
    "X_test_new  = hstack((X_ts, X_ts2, X_ts3))\n",
    "X_train_new.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 36 s\n",
      "Wall time: 20.6 s\n",
      "predict...\n",
      "new essemble\n",
      "('log', 0.26528185760246187)\n",
      "('svm', 0.26195971464540496)\n"
     ]
    }
   ],
   "source": [
    "sgd_log7, sgd_svm7 = fit_SGD(X_train_new,y,'new essemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.277940970765\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_train_cvt1, y, test_size=0.3, \n",
    "                                                     random_state=5, stratify=y)\n",
    "sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 5,  n_jobs = -1)\n",
    "sgd_logit8 = sgd_logit.fit(X_train1, y_train1)\n",
    "pred1 = sgd_logit8.predict_proba(X_valid1)\n",
    "\n",
    "X_train2, X_valid2, y_train1, y_valid1 = train_test_split(X_train_cvt2, y, test_size=0.3, \n",
    "                                                     random_state=5, stratify=y)\n",
    "sgd_logit9 = sgd_logit.fit(X_train2, y_train1)\n",
    "pred2 = sgd_logit9.predict_proba(X_valid2)\n",
    "\n",
    "X_train3, X_valid3, y_train1, y_valid1 = train_test_split(X_train_cvt3, y, test_size=0.3, \n",
    "                                                     random_state=5, stratify=y)\n",
    "sgd_logit10 = sgd_logit.fit(X_train3, y_train1)\n",
    "pred3 = sgd_logit10.predict_proba(X_valid3)\n",
    "\n",
    "X_train4, X_valid4, y_train1, y_valid1 = train_test_split(X_train_new, y, test_size=0.3, \n",
    "                                                     random_state=5, stratify=y)\n",
    "sgd_logit9 = sgd_logit.fit(X_train4, y_train1)\n",
    "pred4 = sgd_logit9.predict_proba(X_valid4)\n",
    "\n",
    "pred_sum_ = 0.28*pred1 + 0.23*pred2 + 0.2*pred3 + 0.27*pred4 \n",
    "\n",
    "pred_sum = sgd_logit9.classes_[pred_sum_.argmax(axis=1)]\n",
    "print accuracy_score(y_valid1, pred_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.277975940691\n"
     ]
    }
   ],
   "source": [
    "pred_sum_ = 0.28*pred1 + 0.23*pred2 + 0.2*pred3 + 0.27*pred4 \n",
    "\n",
    "pred_sum = sgd_logit9.classes_[pred_sum_.argmax(axis=1)]\n",
    "print accuracy_score(y_valid1, pred_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавим временные признаки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319L, 10L), (41177L, 10L), (95319L, 9L), (41177L, 9L))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# время захода на сайт для train_sites, test_sites соответсвенно разница между заходами на сайты в сессии\n",
    "train_time.shape, test_time.shape, train_time_diff.shape, test_time_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# время сессии в 3-х сек\n",
    "train_session_time = (np.max(train_time, axis =1)\n",
    "                                          -np.min(train_time, axis =1))/np.timedelta64(3, 's')\n",
    "test_session_time  = (np.max(test_time, axis =1)\n",
    "                                          -np.min(test_time, axis =1))/np.timedelta64(3, 's')\n",
    "\n",
    "train_session_time = train_session_time.reshape(train_time.shape[0],1)\n",
    "test_session_time  = test_session_time.reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- отмаштабируем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_session_scaled = scale.fit_transform(train_session_time) # время сессии отмаштабированное\n",
    "test_session_scaled  = scale.transform(train_session_time) # время сессии отмаштабированное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "train_time_diff_scaled = scale.fit_transform(train_time_diff)\n",
    "test_time_diff_scaled  = scale.transform(test_time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**начало сессии в часах день недели, время обращения к top300 sites, top10user sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# время начала сессии в часах\n",
    "train_start_hour = (pd.to_datetime(np.min(train_time, axis =1))).hour\n",
    "train_start_hour = train_start_hour.reshape(train_time.shape[0],1)\n",
    "test_start_hour  = (pd.to_datetime(np.min(test_time, axis =1))).hour\n",
    "test_start_hour  = test_start_hour.reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# начало сессии по дням недели\n",
    "train_day_of_week = (pd.to_datetime(np.min(train_time, axis =1))).dayofweek\n",
    "train_day_of_week = train_day_of_week.reshape(train_time.shape[0],1) \n",
    "test_day_of_week = (pd.to_datetime(np.min(test_time, axis =1))).dayofweek\n",
    "test_day_of_week = test_day_of_week.reshape(test_time.shape[0],1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(828, 988)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_top_sites = np.setdiff1d( users_top_sites, top_list_sites)\n",
    "len(diff_top_sites), len(users_top_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319L, 10L), (41177L, 10L))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask_top_sites = in2D(train_sites.drop(['user_id'], axis =1), top_list_sites) #  users_top_sites\n",
    "test_mask_top_sites = in2D(test_sites, top_list_sites) #  users_top_sites\n",
    "train_mask_top_sites.shape, test_mask_top_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_top_site = np.where(train_mask_top_sites, train_time, np.datetime64('nat'))\n",
    "test_time_top_site = np.where(test_mask_top_sites, test_time, np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319L, 10L), (41177L, 10L))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask_top_sites = in2D(train_sites.drop(['user_id'], axis =1), diff_top_sites) #  users_top_sites\n",
    "test_mask_top_sites = in2D(test_sites, diff_top_sites) #  users_top_sites\n",
    "train_mask_top_sites.shape, test_mask_top_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_time_top_user_site = np.where(train_mask_top_sites, train_time, np.datetime64('nat'))\n",
    "test_time_top_user_site = np.where(test_mask_top_sites, test_time, np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время первого захода на сайт из top300 в часах\n",
    "train_top_hour = (pd.to_datetime(np.min(train_time_top_site, axis =1))).hour\n",
    "train_top_hour = np.nan_to_num(train_top_hour).reshape(train_time.shape[0],1)\n",
    "test_top_hour  = (pd.to_datetime(np.min(test_time_top_site, axis =1))).hour\n",
    "test_top_hour  = np.nan_to_num(test_top_hour).reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время первого захода на сайт из top300 в день недели\n",
    "train_top_day = (pd.to_datetime(np.min(train_time_top_site, axis =1))).dayofweek\n",
    "train_top_day = np.nan_to_num(train_top_day).reshape(train_time.shape[0],1)\n",
    "test_top_day  = (pd.to_datetime(np.min(test_time_top_site, axis =1))).dayofweek\n",
    "test_top_day  = np.nan_to_num(test_top_day).reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время первого захода на сайт из top10_user в часах\n",
    "train_top_user_hour = (pd.to_datetime(np.min(train_time_top_user_site, axis =1))).hour\n",
    "train_top_user_hour = np.nan_to_num(train_top_user_hour).reshape(train_time.shape[0],1)\n",
    "test_top_user_hour  = (pd.to_datetime(np.min(test_time_top_user_site, axis =1))).hour\n",
    "test_top_user_hour  = np.nan_to_num(test_top_user_hour).reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время первого захода на сайт из top10_user в день недели\n",
    "train_top_user_day = (pd.to_datetime(np.min(train_time_top_user_site, axis =1))).dayofweek\n",
    "train_top_user_day = np.nan_to_num(train_top_user_day).reshape(train_time.shape[0],1)\n",
    "test_top_user_day  = (pd.to_datetime(np.min(test_time_top_user_site, axis =1))).dayofweek\n",
    "test_top_user_day  = np.nan_to_num(test_top_user_day).reshape(test_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- произведем кодирование временных признаков в категориальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OHE = OneHotEncoder()\n",
    "tr_start_hour_encode = OHE.fit_transform(train_start_hour) # время захода на сайт в часах\n",
    "ts_start_hour_encode = OHE.transform(test_start_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder()\n",
    "tr_start_day_encode = OHE.fit_transform(train_day_of_week) # время захода на сайт в дняхнедели\n",
    "ts_start_day_encode = OHE.transform(test_day_of_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder()\n",
    "tr_top_hour_encode = OHE.fit_transform(train_top_hour.astype(int).astype(str)) # время захода на сайт из top300 в часах\n",
    "ts_top_hour_encode = OHE.transform(test_top_hour.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder()\n",
    "tr_top_day_encode = OHE.fit_transform(train_top_day) # время захода на сайт из top300 в днях\n",
    "ts_top_day_encode = OHE.transform(test_top_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder()\n",
    "tr_top_user_hour_encode = OHE.fit_transform(train_top_user_hour) # время захода на сайт из top10user в часах\n",
    "ts_top_user_hour_encode = OHE.transform(test_top_user_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder()\n",
    "tr_top_user_day_encode = OHE.fit_transform(train_top_user_day) # время захода на сайт из top10user в днях\n",
    "ts_top_user_day_encode = OHE.transform(test_top_user_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим по одному признаку из 6 новых к sites data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 12341), (95319, 17))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt1.shape, tr_start_hour_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12358)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_cvt_time = hstack((X_train_cvt1, tr_start_hour_encode))\n",
    "X_train_cvt_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 34.7 s\n",
      "Wall time: 22.1 s\n",
      "predict...\n",
      "sites + hour in session with CountVectorizer preperation\n",
      "('log', 0.3170373478808225)\n",
      "('svm', 0.3016156105749056)\n"
     ]
    }
   ],
   "source": [
    "sgd_log7, sgd_svm7 = fit_SGD(X_train_cvt_time,y,'sites + hour in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12348)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time2 = hstack((X_train_cvt1, tr_start_day_encode))\n",
    "X_train_cvt_time2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 34.3 s\n",
      "Wall time: 26.8 s\n",
      "predict...\n",
      "sites + hour in session with CountVectorizer preperation\n",
      "('log', 0.31791159602741642)\n",
      "('svm', 0.30056651279899288)\n"
     ]
    }
   ],
   "source": [
    "sgd_log8, sgd_svm8 = fit_SGD(X_train_cvt_time2,y,'sites + hour in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12365)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time2 = hstack((X_train_cvt_time2, tr_start_hour_encode))\n",
    "X_train_cvt_time2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 37.3 s\n",
      "Wall time: 31 s\n",
      "predict...\n",
      "sites + hour+day in session with CountVectorizer preperation\n",
      "('log', 0.35770737166037209)\n",
      "('svm', 0.33875367184221572)\n"
     ]
    }
   ],
   "source": [
    "sgd_log9, sgd_svm9 = fit_SGD(X_train_cvt_time2,y,'sites + hour+day in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12390)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time3 = hstack((X_train_cvt_time2, tr_top_hour_encode, tr_top_day_encode))\n",
    "X_train_cvt_time3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 33 s\n",
      "Wall time: 28.3 s\n",
      "predict...\n",
      "sites + hour+day+top300 in session with CountVectorizer preperation\n",
      "('log', 0.36526087564694365)\n",
      "('svm', 0.33945307035949085)\n"
     ]
    }
   ],
   "source": [
    "sgd_log10, sgd_svm10 = fit_SGD(X_train_cvt_time3,y,'sites + hour+day+top300 in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12426)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time4 = hstack((X_train_cvt_time3, tr_top_user_hour_encode, tr_top_user_day_encode))\n",
    "X_train_cvt_time4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 38.5 s\n",
      "Wall time: 25.5 s\n",
      "predict...\n",
      "sites + hour+day+top300+top10users in session with CountVectorizer preperation\n",
      "('log', 0.36714925164358653)\n",
      "('svm', 0.3426703035389565)\n"
     ]
    }
   ],
   "source": [
    "sgd_log11, sgd_svm11 = fit_SGD(X_train_cvt_time4,y,\\\n",
    "                               'sites + hour+day+top300+top10users in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 24767)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tvt_time6 = hstack((X_train_gramm_tvt,tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_tvt_time6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 38.7 s\n",
      "Wall time: 27.1 s\n",
      "predict...\n",
      "sites + hour+day+top300+top10users in session with TfidfVectorizer preperation\n",
      "('log', 0.32221289690865856)\n",
      "('svm', 0.35634354455168554)\n"
     ]
    }
   ],
   "source": [
    "sgd_log12, sgd_svm12 = fit_SGD(X_train_tvt_time6,y,\\\n",
    "                               'sites + hour+day+top300+top10users in session with TfidfVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_cvt_time4, \n",
    "time_sparse = csr_matrix(train_time_diff_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12435)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time7 = hstack((X_train_cvt_time4, time_sparse))\n",
    "X_train_cvt_time7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 39.5 s\n",
      "Wall time: 31.4 s\n",
      "predict...\n",
      "sites + hour+day+top300+top10users+diff_time in session with CountVectorizer preperation\n",
      "('log', 0.36368722898307454)\n",
      "('svm', 0.34043222828367603)\n"
     ]
    }
   ],
   "source": [
    "sgd_log14, sgd_svm14 = fit_SGD(X_train_cvt_time7,y,\\\n",
    "                               'sites + hour+day+top300+top10users+diff_time in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 12341), (95319, 12341))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt21, X_train_tvt21, X_test_cvt21, X_test_tvt21 = ngramm_analys (train_str,test_str, ngramm =(1,1),\n",
    "                                                                          min_freq =2, analyzer =u'word')\n",
    "X_train_cvt21.shape, X_train_tvt21.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 27265), (95319, 27265))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt32, X_train_tvt32, X_test_cvt32, X_test_tvt32 = ngramm_analys (train_str,test_str, ngramm =(2,2),\n",
    "                                                                          min_freq =3, analyzer =u'word')\n",
    "X_train_cvt32.shape, X_train_tvt32.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 19263), (95319, 19263))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt42, X_train_tvt42, X_test_cvt42, X_test_tvt42 = ngramm_analys (train_str,test_str, ngramm =(2,2),\n",
    "                                                                          min_freq =4, analyzer =u'word')\n",
    "X_train_cvt42.shape, X_train_tvt42.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 24879), (95319, 24879))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt33, X_train_tvt33, X_test_cvt33, X_test_tvt33 = ngramm_analys (train_str,test_str, ngramm =(3,3),\n",
    "                                                                          min_freq =3, analyzer =u'word')\n",
    "X_train_cvt33.shape, X_train_tvt33.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 16358), (95319, 16358))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt43, X_train_tvt43, X_test_cvt43, X_test_tvt43 = ngramm_analys (train_str,test_str, ngramm =(3,3),\n",
    "                                                                          min_freq =4, analyzer =u'word')\n",
    "X_train_cvt43.shape, X_train_tvt43.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 12415)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_cvt_time8 = hstack((X_train_cvt21,tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 33.5 s\n",
      "Wall time: 23.7 s\n",
      "predict...\n",
      "sites+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.37117079311791856)\n",
      "('svm', 0.3410267170233599)\n"
     ]
    }
   ],
   "source": [
    "sgd_log15, sgd_svm15 = fit_SGD(X_train_cvt_time8,y,\\\n",
    "            'sites+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 39680)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time9 = hstack((X_train_cvt21,X_train_cvt32,tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 54.4 s\n",
      "Wall time: 34.3 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.36802349979018045)\n",
      "('svm', 0.3434046719820954)\n"
     ]
    }
   ],
   "source": [
    "sgd_log16, sgd_svm16 = fit_SGD(X_train_cvt_time9,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 31678)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time10 = hstack((X_train_cvt21,X_train_cvt42,tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 48.4 s\n",
      "Wall time: 37.4 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.36777871030913417)\n",
      "('svm', 0.34746118338229121)\n"
     ]
    }
   ],
   "source": [
    "sgd_log17, sgd_svm17 = fit_SGD(X_train_cvt_time10,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 56557)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time11 = hstack((X_train_cvt21,X_train_cvt42, X_train_cvt33, tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 55.9 s\n",
      "Wall time: 46.3 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.3648062666107148)\n",
      "('svm', 0.34441879983214435)\n"
     ]
    }
   ],
   "source": [
    "sgd_log18, sgd_svm18 = fit_SGD(X_train_cvt_time11,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 48036)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time12 = hstack((X_train_cvt21,X_train_cvt42, X_train_cvt43, tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 57 s\n",
      "Wall time: 39.5 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.36473632675898726)\n",
      "('svm', 0.34172611554063503)\n"
     ]
    }
   ],
   "source": [
    "sgd_log19, sgd_svm19 = fit_SGD(X_train_cvt_time12,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 28773)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time13 = hstack((X_train_cvt21,X_train_cvt43, tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 59 s\n",
      "Wall time: 34.9 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.36987690586095956)\n",
      "('svm', 0.34735627360469995)\n"
     ]
    }
   ],
   "source": [
    "sgd_log20, sgd_svm20 = fit_SGD(X_train_cvt_time13,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 21523), (95319, 21523))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt11, X_train_tvt11, X_test_cvt11, X_test_tvt11 = ngramm_analys (train_str,test_str, ngramm =(1,1),\n",
    "                                                                          min_freq =1, analyzer =u'word')\n",
    "X_train_cvt11.shape, X_train_tvt11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 21597)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time14 = hstack((X_train_cvt11, tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time14.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 48.8 s\n",
      "Wall time: 27.9 s\n",
      "predict...\n",
      "sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation\n",
      "('log', 0.37127570289550987)\n",
      "('svm', 0.34190096516995383)\n"
     ]
    }
   ],
   "source": [
    "sgd_log21, sgd_svm21 = fit_SGD(X_train_cvt_time14,y,\\\n",
    "            'sites+ ngramm+ hour+ day+ top300+ top10users  in session with CountVectorizer preperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319, 21597)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvt_time15 = hstack((X_train_tvt11, tr_top_user_hour_encode, tr_top_user_day_encode, \n",
    "            tr_top_hour_encode, tr_top_day_encode, tr_start_hour_encode, tr_start_day_encode))\n",
    "X_train_cvt_time15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 40.2 s\n",
      "Wall time: 25.9 s\n",
      "predict...\n",
      "sites + hour+ day+ top300+ top10users  in session with TfidfVectorizer preperation\n",
      "('log', 0.28098335431528887)\n",
      "('svm', 0.32399636312771019)\n"
     ]
    }
   ],
   "source": [
    "sgd_log22, sgd_svm22 = fit_SGD(X_train_cvt_time15,y,\\\n",
    "            'sites + hour+ day+ top300+ top10users  in session with TfidfVectorizer preperation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз для тестовой выборки с помощью sgd_log21.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41177, 12415)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cvt_time = hstack((X_test_cvt21, ts_top_user_hour_encode, ts_top_user_day_encode, \n",
    "            ts_top_hour_encode, ts_top_day_encode, ts_start_hour_encode, ts_start_day_encode))\n",
    "X_test_cvt_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = sgd_log15.predict(X_test_cvt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file (pred_test, 'kaggle_data/[YDF&MIPT]_Coursera_Oleg67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ансабль моделей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_cvt_time14, y, test_size=0.3, \n",
    "                                                     random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "sgd_logit  = SGDClassifier( loss ='log', random_state = 0,  n_jobs = -1)\n",
    "sgd_svm    = SGDClassifier( loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "sgd_modif  = SGDClassifier( loss ='modified_huber', random_state = 0,  n_jobs = -1)\n",
    "sgd_sq_svm = SGDClassifier( loss ='squared_hinge', random_state=0,  n_jobs = -1)\n",
    " \n",
    "estimators = [('log', sgd_logit), ('svm', sgd_svm), ('modif', sgd_modif), ('sq_svm', sgd_sq_svm)]\n",
    "eclf = VotingClassifier(estimators, voting= 'hard', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log', SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=0, shuffle=True, verbose=0,\n",
       "       warm_s...      penalty='l2', power_t=0.5, random_state=0, shuffle=True, verbose=0,\n",
       "       warm_start=False))],\n",
       "         n_jobs=-1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "eclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VotingClassifier', 0.35665827388445936)\n"
     ]
    }
   ],
   "source": [
    "pred_eclf = eclf.predict(X_valid)\n",
    "print('VotingClassifier',accuracy_score(y_valid, pred_eclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators = [('log', sgd_logit), ('modif', sgd_modif)]\n",
    "eclf2 = VotingClassifier(estimators, voting= 'soft', n_jobs = -1)\n",
    "\n",
    "eclf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VotingClassifier', 0.3503986571548468)\n"
     ]
    }
   ],
   "source": [
    "pred_eclf2 = eclf2.predict(X_valid)\n",
    "print('VotingClassifier',accuracy_score(y_valid, pred_eclf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPC = GaussianProcessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#GPC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создаем разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как ранее. Используем объединенную матрицу train_test_df_sites – потом разделим обратно на обучающую и тестовую части.**\n",
    "\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_sparse_matrix (matrix):\n",
    "    \"\"\"переводим обычную матрицу в разреженноу матрицу \n",
    "    где \n",
    "    номер столбца это уникальное число из исходной матрицы от 1  до максимального\n",
    "    значение в строке это сколько раз уникальное число встречалось в строке оригинальной матрицы\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "        \n",
    "    NMZ = np.prod(np.array(matrix.shape)) # колличество элементов в matrix\n",
    "    data = np.array([1]*NMZ)\n",
    "    indptr = np.arange(0, NMZ+matrix.shape[1], matrix.shape[1])\n",
    "    return csr_matrix((data, matrix.reshape(-1), indptr), dtype=int)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_test_sparse =  matrix_to_sparse_matrix(train_test_df_sites.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 24052)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test_sparse.shape#, matrix_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** добавим к признакам посещенные сайты временные: начало сессии в часах и день недели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 24076)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_test_sparse_time = hstack((X_train_test_sparse, start_hour_encod, day_of_week_encod))#, tr_ts_df_time_encode))\n",
    "X_train_test_sparse_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- разделим train , test данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = X_train_test_sparse_time.tocsr()[: 95319, :]\n",
    "X_test_sparse = X_train_test_sparse_time.tocsr()[95319 : ,:]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 24076), (41177, 24076))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *kaggle_data/train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('kaggle_data/X_train_sparse.pkl', 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl)\n",
    "with open('kaggle_data/X_test_sparse.pkl', 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl)\n",
    "with open('kaggle_data/train_target.pkl', 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_SGD (Train, y, explain = \"fit_SGD\"):\n",
    "    # Разобьем обучающую выборку на 2 части в пропорции 7/3\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size=0.3, \n",
    "                                                     random_state=0, stratify=y)\n",
    "    \n",
    "    sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 0,  n_jobs = -1)\n",
    "    sgd_svm  = SGDClassifier( alpha=0.00007, loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "    print('fit...')\n",
    "    %time sgd_logit.fit(X_train, y_train)\n",
    "    %time sgd_svm.fit(X_train, y_train)\n",
    "    # Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\n",
    "    print('predict...')\n",
    "    pred_log = sgd_logit.predict(X_valid)\n",
    "    pred_svm = sgd_svm.predict(X_valid)\n",
    "    print (explain)\n",
    "    print('log',accuracy_score(y_valid, pred_log))\n",
    "    print('svm',accuracy_score(y_valid, pred_svm))\n",
    "    return sgd_logit, sgd_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы  по признакам site + day + hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 34.1 s\n",
      "Wall time: 26.4 s\n",
      "predict...\n",
      "site + time\n",
      "('log', 0.35812701077073716)\n",
      "('svm', 0.33224926563155688)\n"
     ]
    }
   ],
   "source": [
    "sgd_log, sgd_svm = fit_SGD(X_train_sparse, y, 'site + time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**сделаем класификатор на временных признаках**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 5219)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test_sparse_time = hstack((session_timespan_encode,start_hour_encod, day_of_week_encod, tr_ts_df_time_encode))\n",
    "X_train_test_sparse_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time = X_train_test_sparse_time.tocsr()[: 95319, :]\n",
    "X_test_time = X_train_test_sparse_time.tocsr()[95319 :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы по признакам diff time+ session time + day + hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 36.6 s\n",
      "Wall time: 27.5 s\n",
      "predict...\n",
      "diff time+ session time + day + hour\n",
      "('log', 0.078052874527906005)\n",
      "('svm', 0.034760106308574623)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time, sgd_svm_time = fit_SGD(X_train_time, y, 'diff time+ session time + day + hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**точность классификатора очень низкое**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**составим из ответов этого класификатора и первого новые признаки и обучим на них новую модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pred_time = sgd_log_time.predict_proba(X_train_time) # предсказание временной модели\n",
    "log_pred = sgd_log.predict_proba(X_train_sparse) # предсказание первой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319L, 1100L)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_add = np.hstack((log_pred, log_pred_time) )\n",
    "X_train_add.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы  по признакам first.model + time.model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**точность улучшилась по сравнению с sites+ day + hour  но не значительно**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- загрузим словарь сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fr.openclassrooms.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sigayret.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c1.adform.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dnn506yrbagrg.cloudfront.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ocsp.verisign.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key                          site\n",
       "0    1         fr.openclassrooms.com\n",
       "1    2                   sigayret.fr\n",
       "2    3                 c1.adform.net\n",
       "3    4  dnn506yrbagrg.cloudfront.net\n",
       "4    5             ocsp.verisign.com"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_diction = pd.read_csv('kaggle_data/site_indexes.txt', names =['key','site'])\n",
    "site_diction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим дополнительные признаки по первому и второму доменному именю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>site</th>\n",
       "      <th>_second</th>\n",
       "      <th>_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fr.openclassrooms.com</td>\n",
       "      <td>openclassrooms.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sigayret.fr</td>\n",
       "      <td>sigayret.fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c1.adform.net</td>\n",
       "      <td>adform.net</td>\n",
       "      <td>net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dnn506yrbagrg.cloudfront.net</td>\n",
       "      <td>cloudfront.net</td>\n",
       "      <td>net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ocsp.verisign.com</td>\n",
       "      <td>verisign.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key                          site             _second _first\n",
       "0   1         fr.openclassrooms.com  openclassrooms.com    com\n",
       "1   2                   sigayret.fr         sigayret.fr     fr\n",
       "2   3                 c1.adform.net          adform.net    net\n",
       "3   4  dnn506yrbagrg.cloudfront.net      cloudfront.net    net\n",
       "4   5             ocsp.verisign.com        verisign.com    com"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_diction['_second'] = site_diction.site.apply(lambda x: '.'.join(x.split('.')[-2:]))\n",
    "site_diction['_first'] = site_diction.site.apply(lambda x: x.split('.')[-1])\n",
    "site_diction.astype(str).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- преобразуем в словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_diction = site_diction.astype(str)\n",
    "second_dic = site_diction.set_index('key')['_second'].to_dict()\n",
    "first_dic = site_diction.set_index('key')['_first'].to_dict()\n",
    "second_dic['0'] = ''\n",
    "first_dic['0'] =''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### используем для создания признаков инструменты из text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                    'site6','site7', 'site8', 'site9', 'site10']].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.replace('0','')\n",
    "df_test = df_test.replace('0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_str = df_train.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_str = df_test.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим несколько таблиц признаков с разными значениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы по этим признакам** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-выберем наиболее сильные признаки - ngramm =(1,2) min_df = 2 для log CountVectorizer() сделаем предсказание и будем использовать его в следующем классификаторе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим в качестве признаков первое и второе доменное имя**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = train_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                    'site6','site7', 'site8', 'site9', 'site10']].fillna(0).astype(int).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype(int).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>net</td>\n",
       "      <td>com</td>\n",
       "      <td>net</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>io</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           site1 site2 site3 site4 site5 site6 site7 site8 site9 site10\n",
       "session_id                                                             \n",
       "1            com   com   com    fr   com   com   net   com   net    com\n",
       "2            com   com   com   com   com   com   com   com   com    com\n",
       "3            com    fr    io   com   com    fr    fr   com   com     fr\n",
       "4            com   com   com   com   com   com   com   com   com    com\n",
       "5            org   org   org   org   org    fr    fr   org   org    org"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_train.applymap(lambda x: second_dic[x])\n",
    "df3 = df_train.applymap(lambda x: first_dic[x])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_test = df_test.applymap(lambda x: second_dic[x])\n",
    "df3_test = df_test.applymap(lambda x: first_dic[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train2.apply(lambda x: ' '.join(x) if x != '0' else '', axis =1)\n",
    "df_test2 = df_test2.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.apply(lambda x: ' '.join(x), axis =1)\n",
    "df2_test = df2_test.apply(lambda x: ' '.join(x), axis =1)\n",
    "df3 = df3.apply(lambda x: ' '.join(x), axis =1)\n",
    "df3_test = df3_test.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "cvt = CountVectorizer( ngram_range = (1,2), min_df =2)#, analyzer=u'char')\n",
    "tvt = TfidfVectorizer( ngram_range = (1,2), min_df =2)#, analyzer=u'char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%time X_train_ngram_c = cvt.fit_transform(df_train2.values)\n",
    "X_train_ngram_t = tvt.fit_transform(df_train2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%time X_test_ngram_c = cvt.transform(df_test2.values)\n",
    "X_test_ngram_t = tvt.transform(df_test2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 60252), (95319, 60252))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c.shape, X_train_ngram_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvt = CountVectorizer()\n",
    "tvt = TfidfVectorizer()\n",
    "X_train_ngram_c2 = cvt.fit_transform(df2.values)\n",
    "X_train_ngram_t2 = tvt.fit_transform(df2.values)\n",
    "X_test_ngram_c2 = cvt.transform(df2_test.values)\n",
    "X_test_ngram_t2 = tvt.transform(df2_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 9710), (95319, 9710))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c2.shape, X_train_ngram_t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvt = CountVectorizer()#, analyzer=u'char')\n",
    "tvt = TfidfVectorizer()#, analyzer=u'char')\n",
    "X_train_ngram_c3 = cvt.fit_transform(df3.values)\n",
    "X_train_ngram_t3 = tvt.fit_transform(df3.values)\n",
    "X_test_ngram_c3 = cvt.transform(df3_test.values)\n",
    "X_test_ngram_t3 = tvt.transform(df3_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 144), (95319, 144))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c3.shape, X_train_ngram_t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 70106), (95319, 70106))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new_c = hstack((X_train_ngram_c, X_train_ngram_c2, X_train_ngram_c3))\n",
    "X_train_new_t = hstack((X_train_ngram_t, X_train_ngram_t2, X_train_ngram_t3))\n",
    "X_train_new_c.shape, X_train_new_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 55.8 s\n",
      "Wall time: 37.1 s\n",
      "predict...\n",
      "cvt ngramm + dominate names\n",
      "('log', 0.2470625262274444)\n",
      "('svm', 0.23080151070079732)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new_c, sgd_svm_new_c = fit_SGD(X_train_new_c, y, 'cvt ngramm + dominate names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 46.7 s\n",
      "Wall time: 35.3 s\n",
      "predict...\n",
      "tvt ngramm + dominate names\n",
      "('log', 0.23569730032172331)\n",
      "('svm', 0.30661630997342287)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new_t, sgd_svm_new_t = fit_SGD(X_train_new_t, y, 'tvt ngramm + dominate names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 70106), (95319, 70106))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new2_c = hstack((X_train_ngram_c, X_train_ngram_c2))\n",
    "X_train_new2_t = hstack((X_train_ngram_t, X_train_ngram_t2))\n",
    "X_train_new_c.shape, X_train_new_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 50.2 s\n",
      "Wall time: 37.3 s\n",
      "predict...\n",
      "cvt ngramm + 1 dominate names\n",
      "('log', 0.26409288012309412)\n",
      "('svm', 0.24422996223248006)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new2_c, sgd_svm_new2_c = fit_SGD(X_train_new2_c, y, 'cvt ngramm + 1 dominate names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 58 s\n",
      "Wall time: 43.3 s\n",
      "predict...\n",
      "tvt ngramm + 1 dominate names\n",
      "('log', 0.24674779689467058)\n",
      "('svm', 0.31095258078052873)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new2_t, sgd_svm_new2_t = fit_SGD(X_train_new2_t, y, 'tvt ngramm + 1 dominate names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим временные признаки к ngramm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 17), (136496, 7))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_hour_encod.shape, day_of_week_encod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_hour_train = start_hour_encod.tocsr()[: 95319, :]\n",
    "start_hour_test = start_hour_encod.tocsr()[95319 :, :]\n",
    "day_of_week_train = day_of_week_encod.tocsr()[: 95319, :]\n",
    "day_of_week_test = day_of_week_encod.tocsr()[95319 :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 60276), (95319, 60276))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_time_c = hstack((X_train_ngram_c, start_hour_train, day_of_week_train))\n",
    "X_train_time_t = hstack((X_train_ngram_t, start_hour_train, day_of_week_train))\n",
    "X_train_time_c.shape, X_train_time_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 41.3 s\n",
      "Wall time: 32.8 s\n",
      "predict...\n",
      "cvt ngramm + time\n",
      "('log', 0.35798713106728214)\n",
      "('svm', 0.33676038606798153)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time, sgd_svm_time = fit_SGD(X_train_time_c, y, 'cvt ngramm + time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 46.3 s\n",
      "Wall time: 35.5 s\n",
      "predict...\n",
      "tvt ngramm + time\n",
      "('log', 0.25884739124353057)\n",
      "('svm', 0.36071478528465517)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time2, sgd_svm_time2 = fit_SGD(X_train_time_t, y, 'tvt ngramm + time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new = hstack((X_train_sparse, X_train_ngram))\n",
    "X_test_new = hstack((X_test_sparse, X_test_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty': ('l2', 'l1'), \n",
    "              'alpha': [0.00015, 0.00012, 0.00008, 0.0001]\n",
    "              }           \n",
    "parameters = {'alpha': [0.00007, 0.00006, 0.00009, 0.00012, 0.00008, 0.0001]\n",
    "              }           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(sgd_svm, parameters, cv =skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=7, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=7, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ('l2', 'l1'), 'alpha': [0.00015, 0.00012, 8e-05, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2875919847728669, {'alpha': 8e-05, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30196530983354314"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, clf.best_estimator_ .predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28089264571437134, {'alpha': 0.0001, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer_to_file('{}  {}'.format(round(accuracy_score(y_valid, logit_valid_pred), 3),\n",
    "                                    round(accuracy_score(y_valid, svm_valid_pred), 3)),\n",
    "                     'answer5_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз для тестовой выборки с помощью sgd_logit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = sgd_logit.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = clf.best_estimator_ .predict(X_test_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file (logit_test_pred, 'kaggle_data/[YDF&MIPT]_Coursera_Oleg67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "соревнования, в финальный проект (.pdf или .ipynb).\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1 = train_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clu = DBSCAN(eps=0.6, min_samples=500 )\n",
    "%time clus = clu.fit_predict(matrix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clus, return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\mixture\\base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianGaussianMixture(covariance_prior=None, covariance_type='full',\n",
       "            degrees_of_freedom_prior=None, init_params='kmeans',\n",
       "            max_iter=100, mean_precision_prior=None, mean_prior=None,\n",
       "            n_components=20, n_init=1, random_state=None, reg_covar=1e-06,\n",
       "            tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "            weight_concentration_prior=None,\n",
       "            weight_concentration_prior_type='dirichlet_process')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "BGM = BayesianGaussianMixture(n_components =20)\n",
    "%time BGM.fit(df_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lables = BGM.predict(df_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19], dtype=int64),\n",
       " array([12802,  3332,  3239,  8405,  2428,  3098,  2490,  5426,  2067,\n",
       "         3414,  2800,  6209, 16916,  2866,  5041,  2243,  2183,  2292,\n",
       "         5109,  2959], dtype=int64))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lables, return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
