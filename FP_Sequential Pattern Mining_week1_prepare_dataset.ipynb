{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Идентификация пользователей по посещенным веб-страницам\n",
    "\n",
    "В этом проекте мы будем решать задачу идентификации пользователя по его поведению в сети Интернет. Это сложная и интересная задача на стыке анализа данных и поведенческой психологии. В качестве примера, компания Яндекс решает задачу идентификации взломщика почтового ящика по его поведению. В двух словах, взломщик будет себя вести не так, как владелец ящика: он может не удалять сообщения сразу по прочтении, как это делал хозяин, он будет по-другому ставить флажки сообщениям и даже по-своему двигать мышкой. Тогда такого злоумышленника можно идентифицировать и \"выкинуть\" из почтового ящика, предложив хозяину войти по SMS-коду. Этот пилотный проект описан в [статье](https://habrahabr.ru/company/yandex/blog/230583/) на Хабрахабре. Похожие вещи делаются, например, в Google Analytics и описываются в научных статьях, найти можно многое по фразам \"Traversal Pattern Mining\" и \"Sequential Pattern Mining\".\n",
    "\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "Мы будем решать похожую задачу: по последовательности из нескольких веб-сайтов, посещенных подряд один и тем же человеком, мы будем идентифицировать этого человека. Идея такая: пользователи Интернета по-разному переходят по ссылкам, и это может помогать их идентифицировать (кто-то сначала в почту, потом про футбол почитать, затем новости, контакт, потом наконец – работать, кто-то – сразу работать).\n",
    "\n",
    "Будем использовать данные из [статьи](http://ceur-ws.org/Vol-1058/) \"A Tool for Classification of Sequential Data\". И хотя мы не можем рекомендовать эту статью (описанные методы делеки от state-of-the-art, лучше обращаться к [книге](http://www.charuaggarwal.net/freqbook.pdf) \"Frequent Pattern Mining\" и последним статьям с ICDM), но данные там собраны аккуратно и представляют интерес.\n",
    "\n",
    "Данные собраны с прокси-серверов Университета Блеза Паскаля и имеют очень простой вид: <br>\n",
    "\n",
    "<center>ID пользователя, timestamp, посещенный веб-сайт</center>\n",
    "\n",
    "Скачать исходные данные можно по [ссылке](http://fc.isima.fr/~kahngi/cez13.zip) в статье, там же описание.\n",
    "Для этого задания хватит данных не по всем 3000 пользователям, а по 10 и 150. [Ссылка](https://yadi.sk/d/_HK76ZDo32AvNZ) на архив *capstone_websites_data.zip* (~7.1 Mb, в развернутом виде ~ 65 Mb). \n",
    "\n",
    "В финальном проекте уже придется столкнуться с тем, что не все операции можно выполнить за разумное время (скажем, перебрать с кросс-валидацией 100 комбинаций параметров случайного леса на этих данных Вы вряд ли сможете), поэтому мы будем использовать параллельно 2 выборки: по 10 пользователям и по 150. Для 10 пользователей будем писать и отлаживать код, для 150 – будет рабочая версия. \n",
    "\n",
    "Данные устроены следующем образом:\n",
    "\n",
    " - В каталоге 10users лежат 10 csv-файлов с названием вида \"user[USER_ID].csv\", где [USER_ID] – ID пользователя;\n",
    " - Аналогично для каталога 150users – там 150 файлов;\n",
    " - В 3users_toy – игрушечный пример из 3 файлов, это для отладки кода предобработки, который Вы далее напишете.\n",
    "\n",
    "# <center>Неделя 1. Подготовка данных к анализу и построению моделей\n",
    "\n",
    "Первая часть проекта посвящена подготовке данных для дальнейшего описательного анализа и построения прогнозных моделей. Надо будет написать код для предобработки данных (исходно посещенные веб-сайты указаны для каждого пользователя в отдельном файле) и формирования единой обучающей выборки. Также в этой части мы познакомимся с разреженным форматом данных (матрицы Scipy.sparse), который хорошо подходит для данной задачи. \n",
    "\n",
    "**План 1 недели:**\n",
    " - Часть 1. Подготовка обучающей выборки\n",
    " - Часть 2. Работа с разреженным форматом данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций 1 и 2 недели курса \"Математика и Python для анализа данных\":**\n",
    "   - [Циклы, функции, генераторы, list comprehension](https://www.coursera.org/learn/mathematics-and-python/lecture/Kd7dL/tsikly-funktsii-ghienieratory-list-comprehension)\n",
    "   - [Чтение данных из файлов](https://www.coursera.org/learn/mathematics-and-python/lecture/8Xvwp/chtieniie-dannykh-iz-failov)\n",
    "   - [Запись файлов, изменение файлов](https://www.coursera.org/learn/mathematics-and-python/lecture/vde7k/zapis-failov-izmienieniie-failov)\n",
    "   - [Pandas.DataFrame](https://www.coursera.org/learn/mathematics-and-python/lecture/rcjAW/pandas-data-frame)\n",
    "   - [Pandas. Индексация и селекция](https://www.coursera.org/learn/mathematics-and-python/lecture/lsXAR/pandas-indieksatsiia-i-sieliektsiia)\n",
    "   \n",
    "**Кроме того, в задании будут использоваться библиотеки Python [glob](https://docs.python.org/3/library/glob.html), [pickle](https://docs.python.org/2/library/pickle.html) и класс [csr_matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html) из scipy.sparse.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, для лучшей воспроизводимости результатов приведем список версий основных используемых в проекте библиотек: NumPy, SciPy, Pandas, Matplotlib, Statsmodels и Scikit-learn. Для этого воспользуемся расширением [watermark](https://github.com/rasbt/watermark). Я использую Anaconda3 версии 4.2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%watermark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 2.7.12\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.2\n",
      "scipy 0.18.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.1\n",
      "statsmodels 0.8.0.dev0+302c2ec\n",
      "scikit-learn 0.18.1\n",
      "\n",
      "compiler   : MSC v.1500 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\n",
      "CPU cores  : 2\n",
      "interpreter: 64bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\watermark\\watermark.py:143: DeprecationWarning: Importing scikit-learn as `scikit-learn` has been depracated and will not be supported anymore in v1.7.0. Please use the package name `sklearn` instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,statsmodels,scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на один из файлов с данными о посещенных пользователем (номер 31) веб-страницах.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user31_data = pd.read_csv('capstone_websites_data/10users/user0031.csv', header=None,\n",
    "                  names=['id', 'timestamp', 'site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:17:10</td>\n",
       "      <td>api.dailymotion.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:21:43</td>\n",
       "      <td>b12.myspace.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:21:46</td>\n",
       "      <td>b12.myspace.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:22:14</td>\n",
       "      <td>www.facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-11-15T08:37:30</td>\n",
       "      <td>pixel2368.everesttech.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            timestamp                       site\n",
       "0  31  2013-11-15T08:17:10        api.dailymotion.com\n",
       "1  31  2013-11-15T08:21:43            b12.myspace.com\n",
       "2  31  2013-11-15T08:21:46            b12.myspace.com\n",
       "3  31  2013-11-15T08:22:14           www.facebook.com\n",
       "4  31  2013-11-15T08:37:30  pixel2368.everesttech.net"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Поставим задачу классификации: идентифицировать пользователя по сессии из 10 подряд посещенных сайтов. Объектом в этой задаче будет сессия из 10 сайтов, последовательно посещенных одним и тем же пользователем, признаками – индексы этих 10 сайтов (чуть позже здесь появится \"мешок\" сайтов, подход Bag of Words). Целевым классом будет id пользователя.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Пример для иллюстрации</center>\n",
    "**Пусть пользователя всего 2, длина сессии – 2 сайта.**\n",
    "\n",
    "<center>user1.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">id</th>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:01</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">1</td>\n",
    "    <td class=\"tg-yw4l\">00:00:11</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:16</td>\n",
    "    <td class=\"tg-031e\">vk.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">1</td>\n",
    "    <td class=\"tg-031e\">00:00:20</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<center>user2.csv</center>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">id</th>\n",
    "    <th class=\"tg-031e\">timestamp</th>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:02</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">2</td>\n",
    "    <td class=\"tg-yw4l\">00:00:14</td>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:17</td>\n",
    "    <td class=\"tg-031e\">facebook.com</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">2</td>\n",
    "    <td class=\"tg-031e\">00:00:25</td>\n",
    "    <td class=\"tg-031e\">yandex.ru</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Идем по 1 файлу, нумеруем сайты подряд: vk.com – 1, google.com – 2 и т.д. Далее по второму файлу. \n",
    "\n",
    "Отображение сайтов в их индесы должно получиться таким:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">site</th>\n",
    "    <th class=\"tg-yw4l\">site_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">vk.com</td>\n",
    "    <td class=\"tg-yw4l\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">google.com</td>\n",
    "    <td class=\"tg-yw4l\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">yandex.ru</td>\n",
    "    <td class=\"tg-yw4l\">3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">facebook.com</td>\n",
    "    <td class=\"tg-yw4l\">4</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Тогда обучающая выборка будет такой:\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-s6z2{text-align:center}\n",
    ".tg .tg-baqh{text-align:center;vertical-align:top}\n",
    ".tg .tg-hgcj{font-weight:bold;text-align:center}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-hgcj\">session_id</th>\n",
    "    <th class=\"tg-hgcj\">site1</th>\n",
    "    <th class=\"tg-hgcj\">site2</th>\n",
    "    <th class=\"tg-amwm\">target</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-s6z2\">1</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-s6z2\">2</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">4</td>\n",
    "    <td class=\"tg-s6z2\">3</td>\n",
    "    <td class=\"tg-baqh\">2</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Здесь 1 объект – это сессия из 2 посещенных сайтов 1-ым пользователем (target=1). Это сайты vk.com и google.com (номер 1 и 2). И так далее, всего 4 сессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка обучающей выборки\n",
    "Реализуйте функцию *prepare_train_set*, которая принимает на вход путь к каталогу с csv-файлами и параметр *session_length* – длину сессии, а возвращает 2 объекта:\n",
    "- матрицу (двухмерный NumPy array), в которой строки соответствуют сессиям из *session_length* сайтов, *session_length* столбцов – индексам этих *session_length* сайтов и последний столбец – ID пользователя. \n",
    "- частотный словарь сайтов вида {'site_string': (site_id, site_freq)}, например для недавнего игрушечного примера это будет {'vk.com': (1, 2), 'google.com': (2, 2), 'yandex.ru': (3, 3), 'facebook.com': (4, 1)}\n",
    "\n",
    "Детали:\n",
    "- Используйте glob (или аналоги) для обхода файлов в каталоге\n",
    "- Создайте частотный словарь уникальных сайтов (вида {'site_string': (site_id, site_freq)}) и заполняйте его по ходу чтения файлов. Начинайте с 1, то есть первый попавшийся сайт кодируйте единицей\n",
    "- Не делайте entity recognition, считайте *google.com*, *http://www.google.com* и *www.google.com* разными сайтами\n",
    "- Скорее всего в файле число записей не кратно числу *session_length*. Тогда последняя сессия будет короче. Остаток заполняйте нулями. То есть если в файле 24 записи и сессии длины 10, то 3 сессия будет состоять из 4 сайтов, и ей мы сопоставим вектор [*site1_id*, *site2_id*, *site3_id*, *site4_id*, 0, 0, 0, 0, 0, 0, *user_id*] \n",
    "- 150 файлов из *capstone_websites_data/150users/* должны обрабатываться примерно за 30 секунд, но многое, конечно, зависит от реализации функции и от используемого железа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_set(csv_files_mask, session_length):\n",
    "    '''\n",
    "    input \n",
    "    csv_files_mask = dir of session files,\n",
    "    session_length = size of the session\n",
    "    output \n",
    "    matrix - array where rows are sessions, columns are indexes of site_id, last column is user_id (target)\n",
    "    '''   \n",
    "    from glob import glob \n",
    "    \n",
    "    data = pd.DataFrame(columns = None, dtype = np.int64)\n",
    "    dic = {} # dictionary where keys are site names values are (site_id, frequency of site_id)\n",
    "    i = 1 # indicator of sites - site_id\n",
    "    \n",
    "    # from csv_files_mask to read all files\n",
    "    for f in glob(csv_files_mask):\n",
    "        df = pd.read_csv( f, header=None, names=['id', 'timestamp', 'site']) # make the dataframe from file\n",
    "        # array with site_id  from dataframe now it is initialed  zeros length is multiple to the session_length\n",
    "        ar = np.zeros((np.ceil(len(df)/session_length)*session_length, 1), dtype=np.int64) \n",
    "        \n",
    "        \n",
    "        for j,x in enumerate (df.site):\n",
    "            # empty dictionary and array \n",
    "            if x in dic.keys():\n",
    "                dic[x] = (dic[x][0], dic[x][1]+1) \n",
    "            else:\n",
    "                dic[x]=(i,1)\n",
    "                i +=1\n",
    "            ar[j] = dic[x][0] # \n",
    "        \n",
    "        ar = ar.reshape((np.ceil(len(df)/session_length),session_length)) # convert array to matrix\n",
    "        \n",
    "        df1 = pd.DataFrame(ar, columns = None, dtype = np.int64) # convert array to datafarme\n",
    "        df1['target'] = df['id']  # add target column\n",
    "        \n",
    "        data = data.append(df1) # add dataframe from next file  in dir csv_files_mask\n",
    "        \n",
    "    return data.values, dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените полученную функцию к игрушечному примеру, убедитесь, что все работает как надо. Это можно сделать просто глазами (подход eye-balling), а также проверив 2 выражения assert ниже. Если при  выполнении это клетки ничего не произойдет, то скорее функция реализована привильно, а возникновение AssertionError говорит о проблемах с *prepare_train_set*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_toy, site_freq_3users = prepare_train_set('capstone_websites_data/3users_toy/*', \n",
    "                                                     session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2,  3,  2,  4,  5,  6,  7,  8,  1],\n",
       "       [ 1,  4,  4,  4,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 1,  2,  9,  9,  2,  0,  0,  0,  0,  0,  2],\n",
       "       [10,  4,  2,  4,  2,  4,  4,  6, 11, 10,  3],\n",
       "       [10,  4,  2,  0,  0,  0,  0,  0,  0,  0,  3]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounts.google.com': (5, 1),\n",
       " 'apis.google.com': (7, 1),\n",
       " 'football.kulichki.ru': (9, 2),\n",
       " 'geo.mozilla.org': (3, 1),\n",
       " 'google.com': (4, 9),\n",
       " 'mail.google.com': (6, 2),\n",
       " 'meduza.io': (10, 3),\n",
       " 'oracle.com': (2, 8),\n",
       " 'plus.google.com': (8, 1),\n",
       " 'vk.com': (1, 3),\n",
       " 'yandex.ru': (11, 1)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_freq_3users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert train_data_toy.sum() == 159\n",
    "assert site_freq_3users['meduza.io'] == (10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Примените полученную функцию к данным по 10 пользователям, посмотрите, сколько уникальных сессий из 10 сайтов получится, запишите ответ в файл с помощью функции write_answer_to_file – это будет ответом к первому вопросу теста. Если ответ получился неправильный, значит где-то ошибка в функции prepare_train_set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%time train_data_10users, site_freq_10users = prepare_train_set('capstone_websites_data/10users/*', session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_answer_to_file(answer, file_address):\n",
    "    with open(file_address, 'w') as out_f:\n",
    "        out_f.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14061L, 11L), 14061)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_10users.shape, len(train_data_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write_answer_to_file(len(train_data_10users), 'answer1_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Сколько всего уникальных сайтов в выборке из 10 пользователей? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4913"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write_answer_to_file(len(site_freq_10users), 'answer1_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Примените полученную функцию к данным по 150 пользователям, посмотрите, сколько уникальных сессий из 10 сайтов получится, запишите ответ в файл с помощью функции write_answer_to_file – это будет ответом к третьему вопросу. Если ответ получился неправильный, значит где-то ошибка в функции prepare_train_set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 39s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time train_data_150users, site_freq_150users = prepare_train_set('capstone_websites_data/150users/*.csv',session_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137019, 27797)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_150users), len(site_freq_150users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write_answer_to_file(len(train_data_150users), 'answer1_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Сколько всего уникальных сайтов в выборке из 150 пользователей? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write_answer_to_file(len(site_freq_150users), 'answer1_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Выведите топ-10 самых популярных сайтов среди посещенных 150 пользователями. Запишите ответ в файл *answer1_5.txt* через пробел – все 10 сайтов на одной строке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b =site_freq_10users.items() # convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b.sort(key =lambda x: x[1][1], reverse=True) # sort list by frequensy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s.youtube.com www.google.fr www.google.com mail.google.com www.facebook.com apis.google.com r3---sn-gxo5uxg-jqbe.googlevideo.com r1---sn-gxo5uxg-jqbe.googlevideo.com plus.google.com accounts.google.com'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([x[0] for x in b[:10]]) # top 10 by frequensy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = site_freq_150users.items()\n",
    "b.sort(key =lambda x: x[1][1], reverse=True)\n",
    "top10_popular = [x[0] for x in b[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.google.fr',\n",
       " 'www.google.com',\n",
       " 'www.facebook.com',\n",
       " 'apis.google.com',\n",
       " 's.youtube.com',\n",
       " 'clients1.google.com',\n",
       " 'mail.google.com',\n",
       " 'plus.google.com',\n",
       " 'safebrowsing-cache.google.com',\n",
       " 'www.youtube.com']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write_answer_to_file(\" \".join(top10_popular), 'answer1_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.google.fr www.google.com www.facebook.com apis.google.com s.youtube.com clients1.google.com mail.google.com plus.google.com safebrowsing-cache.google.com www.youtube.com'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(top10_popular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для дальнейшего анализа превратим полученные матрицы опять в DataFrame и запишем в csv-файлы.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_10users = pd.DataFrame(train_data_10users, \n",
    "                        columns=['site' + str(i) for i in range(1,11)] + ['target'])\n",
    "train_df_150users = pd.DataFrame(train_data_150users, \n",
    "                        columns=['site' + str(i) for i in range(1,11)] + ['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним полученные DataFrame в csv-файлы, при этом учтем, что все признаки у нас целочисленные, поэтому запишем их в соответствующем формате: у DataFrame.to_csv аргумент float_format='%d'. Так на одних только символах разделителя и последующего нуля можно сэкономить почти 30% места.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_10users.to_csv('capstone_websites_data/train_data_10users.csv', \n",
    "                        index_label='session_id', float_format='%d')\n",
    "train_df_150users.to_csv('capstone_websites_data/train_data_150users.csv', \n",
    "                         index_label='session_id', float_format='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Работа с разреженным форматом данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Если так подумать, то полученные признаки site1, ..., site10 смысла не имеют как признаки в задаче классификации. А вот если воспользоваться идеей мешка слов из анализа текстов – это другое дело. Создадим новые матрицы, в которых строкам будут соответствовать сессии из 10 сайтов, а столбцам – индексы сайтов. На пересечении строки $i$ и столбца $j$ будет стоять число $n_{ij}$ – cколько раз сайт $j$ встретился в сессии номер $i$. Делать это будем с помощью разреженных матриц Scipy – [csr_matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html). Прочитайте документацию, разберитесь, как использовать разреженные матрицы и создайте такие матрицы для наших данных. Сначала проверьте на игрушечном примере, затем примените для 10 и 150 пользователей. **\n",
    "\n",
    "**Обратите внимание, что в коротких сессиях, меньше 10 сайтов, у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_toy, y_toy = train_data_toy[:, :-1], train_data_toy[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2,  3,  2,  4,  5,  6,  7,  8],\n",
       "       [ 1,  4,  4,  4,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  2,  9,  9,  2,  0,  0,  0,  0,  0],\n",
       "       [10,  4,  2,  4,  2,  4,  4,  6, 11, 10],\n",
       "       [10,  4,  2,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_sparse_matrix (matrix):\n",
    "    \"\"\"переводим обычную матрицу в разреженноу матрицу \n",
    "    где \n",
    "    номер столбца это уникальное число из исходной матрицы от 1  до максимального\n",
    "    значение в строке это сколько раз уникальное число встречалось в строке оригинальной матрицы\"\"\"\n",
    "    from scipy.sparse import csr_matrix\n",
    "    \n",
    "    NMZ = np.prod(np.array(matrix.shape)) # колличество элементов в matrix\n",
    "    data = np.array([1]*NMZ)\n",
    "    indptr = np.arange(0, NMZ+matrix.shape[1], matrix.shape[1])\n",
    "    return csr_matrix((data, matrix.reshape(-1), indptr), dtype=int)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sparse_toy = matrix_to_sparse_matrix(X_toy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0],\n",
       "        [0, 2, 0, 4, 0, 1, 0, 0, 0, 2, 1],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_toy.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "0               1      2      2      3      4      5      6      7      8   \n",
       "1               7      8      7      7      9     10     11     12     13   \n",
       "2               7     15     16     17     17      8     17     17     10   \n",
       "3              18     19     20     17     21     17     17     17     17   \n",
       "4              23     24     25     17     26     27     28     29     30   \n",
       "\n",
       "            site10  target  \n",
       "session_id                  \n",
       "0                7      31  \n",
       "1               14      31  \n",
       "2                3      31  \n",
       "3               22      31  \n",
       "4               31      31  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_10users = pd.read_csv('capstone_websites_data/train_data_10users.csv', index_col='session_id')\n",
    "train_df_10users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_10users, y_10users = train_df_10users.ix[:, :-1].values, train_df_10users.ix[:, -1].values\n",
    "#X_150users, y_150users = train_data_150users[:, :-1], train_data_150users[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21 ms\n"
     ]
    }
   ],
   "source": [
    "%time X_sparse_10users =  matrix_to_sparse_matrix(X_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "0               1      2      3      2      3      1      4      5      6   \n",
       "1               1      1      4      1      1      1      7      1      1   \n",
       "2               8      1      9     10      9     10     11      8      9   \n",
       "3              12     13      8      9     12     13     11      8      9   \n",
       "4              10     12      8     11      9     13     13     12      8   \n",
       "\n",
       "            site10  target  \n",
       "session_id                  \n",
       "0                6       6  \n",
       "1                1       6  \n",
       "2               10       6  \n",
       "3               10       6  \n",
       "4               11       6  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_150users = pd.read_csv('capstone_websites_data/train_data_150users.csv', index_col='session_id')\n",
    "train_df_150users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_150users, y_150users = train_df_150users.ix[:, :-1].values, train_df_150users.ix[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%time X_sparse_150users =  matrix_to_sparse_matrix(X_150users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним эти разреженные матрицы с помощью [pickle](https://docs.python.org/2/library/pickle.html) (сериализация в Python), также сохраним вектора y_10users, y_150users – целевые значения (id пользователя)  в выборках из 10 и 150 пользователей. То что названия этих матриц начинаются с X и y, намекает на то, что на этих данных мы будем проверять первые модели классификации.\n",
    "Наконец, сохраним также и частотные словари сайтов для 3, 10 и 150 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('capstone_websites_data/X_sparse_10users.pkl', 'wb') as X10_pkl:\n",
    "    pickle.dump(X_sparse_10users, X10_pkl)\n",
    "with open('capstone_websites_data/y_10users.pkl', 'wb') as y10_pkl:\n",
    "    pickle.dump(y_10users, y10_pkl)\n",
    "#with open('capstone_websites_data/site_freq_10users.pkl', 'wb') as site_freq_10users_pkl:\n",
    "#    pickle.dump(site_freq_10users, site_freq_10users_pkl)    \n",
    "with open('capstone_websites_data/X_sparse_150users.pkl', 'wb') as X150_pkl:\n",
    "    pickle.dump(X_sparse_150users, X150_pkl)\n",
    "with open('capstone_websites_data/y_150users.pkl', 'wb') as y150_pkl:\n",
    "    pickle.dump(y_150users, y150_pkl)\n",
    "#with open('capstone_websites_data/site_freq_150users.pkl', 'wb') as site_freq_150users_pkl:\n",
    "#    pickle.dump(site_freq_150users, site_freq_150users_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Убедимся, что число столбцов в разреженных матрицах X_sparse_10users и X_sparse_150users равно ранее посчитанным числам уникальных сайтов для 10 и 150 пользователей соответственно. Важно удостовериться, что потом мы все будем работать с одними и теми же данными.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert X_sparse_10users.shape[1] == len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14061, 4913), 4913)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_10users.shape, len(site_freq_10users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert X_sparse_150users.shape[1] == len(site_freq_150users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137019, 27797)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sparse_150users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пути улучшения\n",
    "В этом проекте свобода творчества на каждом шаге, а 6 и 7 недели проекта посвящены общему описанию проекта (.ipynb или pdf) и взаимному оцениванию. Что еще можно добавить по первой части проекта:\n",
    "- при наличии доступа к хорошим мощностям (можно арендовать инстанс Amazon EC2, как именно, описано [тут](https://habrahabr.ru/post/280562/)) можно обработать исходные данные по 3000 пользователей"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
