{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Identify Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/data) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle_data/train_sessions.csv', index_col='session_id')\n",
    "test_df = pd.read_csv('kaggle_data/test_sessions.csv', index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>932.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>151.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3191.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>2014-02-14 15:16:45</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:17:13</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2014-02-14 15:20:47</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:21:13</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1943</td>\n",
       "      <td>2014-03-17 15:19:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:20:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:21:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:42</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:43</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2   site3  \\\n",
       "session_id                                                                    \n",
       "1               8  2014-01-04 08:44:50    11.0  2014-01-04 08:44:50    82.0   \n",
       "2             111  2014-03-18 10:33:20    78.0  2014-03-18 10:33:31   151.0   \n",
       "3              11  2014-12-02 13:13:41  3187.0  2014-12-02 13:13:41   132.0   \n",
       "4             668  2014-02-14 15:16:45  1965.0  2014-02-14 15:17:13   598.0   \n",
       "5            1943  2014-03-17 15:19:40  1943.0  2014-03-17 15:20:10  1943.0   \n",
       "\n",
       "                          time3   site4                time4   site5  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:19    68.0  2014-01-04 08:45:25     8.0   \n",
       "2           2014-03-18 10:33:31   111.0  2014-03-18 10:33:31  1401.0   \n",
       "3           2014-12-02 13:13:42   496.0  2014-12-02 13:13:42  1969.0   \n",
       "4           2014-02-14 15:20:47  1965.0  2014-02-14 15:21:13   284.0   \n",
       "5           2014-03-17 15:21:40  1943.0  2014-03-17 15:22:10  1943.0   \n",
       "\n",
       "                          time5   ...                  time6   site7  \\\n",
       "session_id                        ...                                  \n",
       "1           2014-01-04 08:45:25   ...    2014-01-04 08:45:51  8403.0   \n",
       "2           2014-03-18 10:33:31   ...    2014-03-18 10:33:32  1375.0   \n",
       "3           2014-12-02 13:13:45   ...    2014-12-02 13:13:45  3187.0   \n",
       "4           2014-02-14 15:21:14   ...    2014-02-14 15:21:14    38.0   \n",
       "5           2014-03-17 15:22:39   ...    2014-03-17 15:22:39  1952.0   \n",
       "\n",
       "                          time7   site8                time8   site9  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:51   932.0  2014-01-04 08:45:53  3260.0   \n",
       "2           2014-03-18 10:33:32    38.0  2014-03-18 10:33:32  1401.0   \n",
       "3           2014-12-02 13:13:45    82.0  2014-12-02 13:13:46  3191.0   \n",
       "4           2014-02-14 15:21:14  4451.0  2014-02-14 15:21:14  4537.0   \n",
       "5           2014-03-17 15:22:41  1943.0  2014-03-17 15:22:41  1943.0   \n",
       "\n",
       "                          time9  site10               time10 user_id  \n",
       "session_id                                                            \n",
       "1           2014-01-04 08:45:53     8.0  2014-01-04 08:45:53    1845  \n",
       "2           2014-03-18 10:33:32    97.0  2014-03-18 10:33:34    3322  \n",
       "3           2014-12-02 13:13:46  3184.0  2014-12-02 13:13:47    2003  \n",
       "4           2014-02-14 15:21:15    11.0  2014-02-14 15:21:15    1373  \n",
       "5           2014-03-17 15:22:42  1943.0  2014-03-17 15:22:43    1737  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 21), (41177, 20), (136496, 21))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 550 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза ID пользователя будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>8393</td>\n",
       "      <td>8403</td>\n",
       "      <td>932</td>\n",
       "      <td>3260</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>78</td>\n",
       "      <td>151</td>\n",
       "      <td>111</td>\n",
       "      <td>1401</td>\n",
       "      <td>151</td>\n",
       "      <td>1375</td>\n",
       "      <td>38</td>\n",
       "      <td>1401</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>3187</td>\n",
       "      <td>132</td>\n",
       "      <td>496</td>\n",
       "      <td>1969</td>\n",
       "      <td>504</td>\n",
       "      <td>3187</td>\n",
       "      <td>82</td>\n",
       "      <td>3191</td>\n",
       "      <td>3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>1965</td>\n",
       "      <td>598</td>\n",
       "      <td>1965</td>\n",
       "      <td>284</td>\n",
       "      <td>668</td>\n",
       "      <td>38</td>\n",
       "      <td>4451</td>\n",
       "      <td>4537</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "      <td>1952</td>\n",
       "      <td>1952</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2941</td>\n",
       "      <td>2951</td>\n",
       "      <td>2958</td>\n",
       "      <td>2993</td>\n",
       "      <td>162</td>\n",
       "      <td>2967</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>2967</td>\n",
       "      <td>2967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9996</td>\n",
       "      <td>9996</td>\n",
       "      <td>307</td>\n",
       "      <td>9996</td>\n",
       "      <td>280</td>\n",
       "      <td>9996</td>\n",
       "      <td>9996</td>\n",
       "      <td>307</td>\n",
       "      <td>9996</td>\n",
       "      <td>9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19823</td>\n",
       "      <td>19823</td>\n",
       "      <td>1510</td>\n",
       "      <td>32</td>\n",
       "      <td>19808</td>\n",
       "      <td>8</td>\n",
       "      <td>567</td>\n",
       "      <td>567</td>\n",
       "      <td>654</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>561</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>329</td>\n",
       "      <td>329</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1               8     11     82     68      8   8393   8403    932   3260   \n",
       "2             111     78    151    111   1401    151   1375     38   1401   \n",
       "3              11   3187    132    496   1969    504   3187     82   3191   \n",
       "4             668   1965    598   1965    284    668     38   4451   4537   \n",
       "5            1943   1943   1943   1943   1943   1952   1952   1943   1943   \n",
       "6            2941   2951   2958   2993    162   2967     85      7   2967   \n",
       "7            9996   9996    307   9996    280   9996   9996    307   9996   \n",
       "8           19823  19823   1510     32  19808      8    567    567    654   \n",
       "9              72      0      0      0      0      0      0      0      0   \n",
       "10             32     32    561     69     32    329    329     69     69   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1                8  \n",
       "2               97  \n",
       "3             3184  \n",
       "4               11  \n",
       "5             1943  \n",
       "6             2967  \n",
       "7             9996  \n",
       "8              567  \n",
       "9                0  \n",
       "10             329  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создадим признаки связанные с временем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_df[['time1', 'time2', 'time3', \n",
    "                                     'time4','time5', 'time6',\n",
    "                                     'time7','time8', 'time9', \n",
    "                                     'time10']].fillna(np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_df_time.apply(pd.to_datetime).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496L, 10L)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# время на сайте в сек\n",
    "train_test_df_time_diff = np.diff(train_test_df_time, axis =1)/np.timedelta64(3, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_time_diff = np.nan_to_num(train_test_df_time_diff) # заменим пропуски на ноль"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- отмаштабируем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#tr_ts_df_time_scaled = StandardScaler().fit_transform(train_test_df_time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время сессии в 3-х сек\n",
    "session_timespan = (np.max(train_test_df_time, axis =1)\n",
    "                                          -np.min(train_test_df_time, axis =1))/np.timedelta64(3, 's')\n",
    "\n",
    "session_timespan = session_timespan.reshape(train_test_df_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600.0, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_timespan.max(), session_timespan.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_timespan_scaled = StandardScaler().fit_transform(session_timespan) # время сессии отмаштабированное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# время начала сессии в часах\n",
    "start_hour = (pd.to_datetime(np.min(train_test_df_time, axis =1))).hour\n",
    "start_hour = start_hour.reshape(train_test_df_time.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- произведем кодирование временных признаков в категориальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "tr_ts_df_time_encode = OneHotEncoder().fit_transform(train_test_df_time_diff) # перевод категориальных признаков в двоичные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 601), (136496, 4594))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_timespan_encode = OneHotEncoder().fit_transform(session_timespan)\n",
    "session_timespan_encode.shape, tr_ts_df_time_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# начало сессии в часах\n",
    "start_hour_encod = OneHotEncoder().fit_transform(start_hour) # перевод категориальных признаков в двоичные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# начало сессии по дням недели\n",
    "day_of_week = (pd.to_datetime(np.min(train_test_df_time, axis =1))).dayofweek\n",
    "day_of_week = day_of_week.reshape(train_test_df_time.shape[0],1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_of_week_encod = OneHotEncoder().fit_transform(day_of_week) # перевод категориальных признаков в двоичные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 601), (136496, 17), (136496, 7))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_timespan_encode.shape, start_hour_encod.shape,  day_of_week_encod.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу train_test_df_sites – потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_sparse_matrix (matrix):\n",
    "    \"\"\"переводим обычную матрицу в разреженноу матрицу \n",
    "    где \n",
    "    номер столбца это уникальное число из исходной матрицы от 1  до максимального\n",
    "    значение в строке это сколько раз уникальное число встречалось в строке оригинальной матрицы\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "        \n",
    "    NMZ = np.prod(np.array(matrix.shape)) # колличество элементов в matrix\n",
    "    data = np.array([1]*NMZ)\n",
    "    indptr = np.arange(0, NMZ+matrix.shape[1], matrix.shape[1])\n",
    "    return csr_matrix((data, matrix.reshape(-1), indptr), dtype=int)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_test_sparse =  matrix_to_sparse_matrix(train_test_df_sites.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 24052)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test_sparse.shape#, matrix_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** добавим к признакам посещенные сайты временные: начало сессии в часах и день недели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 24076)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_test_sparse_time = hstack((X_train_test_sparse, start_hour_encod, day_of_week_encod))#, tr_ts_df_time_encode))\n",
    "X_train_test_sparse_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- разделим train , test данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sparse = X_train_test_sparse[: 95319, :]\n",
    "X_test_sparse = X_train_test_sparse[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 24076), (41177, 24076))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *kaggle_data/train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('kaggle_data/X_train_sparse.pkl', 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl)\n",
    "with open('kaggle_data/X_test_sparse.pkl', 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl)\n",
    "with open('kaggle_data/train_target.pkl', 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создайте объекты sklearn.linear_model.SGDClassifier с логистической функцией потерь и с hinge loss (логистическая регрессия и линейный SVM соответственно) . Остальные параметры  по умолчанию, разве что alpha=0.00007, n_jobs=-1 никогда не помешает. Обучите  модели на выборке (X_train, y_train).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_SGD (Train, y, explain = \"fit_SGD\"):\n",
    "    # Разобьем обучающую выборку на 2 части в пропорции 7/3\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(Train, y, test_size=0.3, \n",
    "                                                     random_state=0, stratify=y)\n",
    "    \n",
    "    sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 0,  n_jobs = -1)\n",
    "    sgd_svm  = SGDClassifier( alpha=0.00007, loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "    print('fit...')\n",
    "    %time sgd_logit.fit(X_train, y_train)\n",
    "    %time sgd_svm.fit(X_train, y_train)\n",
    "    # Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\n",
    "    print('predict...')\n",
    "    pred_log = sgd_logit.predict(X_valid)\n",
    "    pred_svm = sgd_svm.predict(X_valid)\n",
    "    print (explain)\n",
    "    print('log',accuracy_score(y_valid, pred_log))\n",
    "    print('svm',accuracy_score(y_valid, pred_svm))\n",
    "    return sgd_logit, sgd_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы  по признакам site + day + hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 34.1 s\n",
      "Wall time: 26.4 s\n",
      "predict...\n",
      "site + time\n",
      "('log', 0.35812701077073716)\n",
      "('svm', 0.33224926563155688)\n"
     ]
    }
   ],
   "source": [
    "sgd_log, sgd_svm = fit_SGD(X_train_sparse, y, 'site + time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**сделаем класификатор на временных признаках**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 5219)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test_sparse_time = hstack((session_timespan_encode,start_hour_encod, day_of_week_encod, tr_ts_df_time_encode))\n",
    "X_train_test_sparse_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_time = X_train_test_sparse_time.tocsr()[: 95319, :]\n",
    "X_test_time = X_train_test_sparse_time.tocsr()[95319 :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы по признакам diff time+ session time + day + hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 36.6 s\n",
      "Wall time: 27.5 s\n",
      "predict...\n",
      "diff time+ session time + day + hour\n",
      "('log', 0.078052874527906005)\n",
      "('svm', 0.034760106308574623)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time, sgd_svm_time = fit_SGD(X_train_time, y, 'diff time+ session time + day + hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**точность классификатора очень низкое**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**составим из ответов этого класификатора и первого новые признаки и обучим на них новую модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_pred_time = sgd_log_time.predict_proba(X_train_time) # предсказание временной модели\n",
    "log_pred = sgd_log.predict_proba(X_train_sparse) # предсказание первой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95319L, 1100L)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_add = np.hstack((log_pred, log_pred_time) )\n",
    "X_train_add.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы  по признакам first.model + time.model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ea40da62e566>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msgd_log_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd_svm_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'first.model + time.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-ebbe48383c9b>\u001b[0m in \u001b[0;36mfit_SGD\u001b[1;34m(Train, y, explain)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msgd_svm\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00007\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'hinge'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time sgd_logit.fit(X_train, y_train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time sgd_svm.fit(X_train, y_train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    543\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate, self.n_iter,\n\u001b[1;32m--> 415\u001b[1;33m                           classes, sample_weight, coef_init, intercept_init)\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, n_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    371\u001b[0m             self._fit_multiclass(X, y, alpha=alpha, C=C,\n\u001b[0;32m    372\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                                  sample_weight=sample_weight, n_iter=n_iter)\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             self._fit_binary(X, y, alpha=alpha, C=C,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.pyc\u001b[0m in \u001b[0;36m_fit_multiclass\u001b[1;34m(self, X, y, alpha, C, learning_rate, sample_weight, n_iter)\u001b[0m\n\u001b[0;32m    455\u001b[0m                                 \u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m                                 sample_weight)\n\u001b[1;32m--> 457\u001b[1;33m             for i in range(len(self.classes_)))\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sgd_log_add, sgd_svm_add = fit_SGD(X_train_add, y, 'first.model + time.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**точность улучшилась по сравнению с sites+ day + hour  но не значительно**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- загрузим словарь сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fr.openclassrooms.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sigayret.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c1.adform.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dnn506yrbagrg.cloudfront.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ocsp.verisign.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key                          site\n",
       "0    1         fr.openclassrooms.com\n",
       "1    2                   sigayret.fr\n",
       "2    3                 c1.adform.net\n",
       "3    4  dnn506yrbagrg.cloudfront.net\n",
       "4    5             ocsp.verisign.com"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_diction = pd.read_csv('kaggle_data/site_indexes.txt', names =['key','site'])\n",
    "site_diction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим дополнительные признаки по первому и второму доменному именю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>site</th>\n",
       "      <th>_second</th>\n",
       "      <th>_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fr.openclassrooms.com</td>\n",
       "      <td>openclassrooms.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sigayret.fr</td>\n",
       "      <td>sigayret.fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c1.adform.net</td>\n",
       "      <td>adform.net</td>\n",
       "      <td>net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dnn506yrbagrg.cloudfront.net</td>\n",
       "      <td>cloudfront.net</td>\n",
       "      <td>net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ocsp.verisign.com</td>\n",
       "      <td>verisign.com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key                          site             _second _first\n",
       "0   1         fr.openclassrooms.com  openclassrooms.com    com\n",
       "1   2                   sigayret.fr         sigayret.fr     fr\n",
       "2   3                 c1.adform.net          adform.net    net\n",
       "3   4  dnn506yrbagrg.cloudfront.net      cloudfront.net    net\n",
       "4   5             ocsp.verisign.com        verisign.com    com"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_diction['_second'] = site_diction.site.apply(lambda x: '.'.join(x.split('.')[-2:]))\n",
    "site_diction['_first'] = site_diction.site.apply(lambda x: x.split('.')[-1])\n",
    "site_diction.astype(str).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- преобразуем в словари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_diction = site_diction.astype(str)\n",
    "second_dic = site_diction.set_index('key')['_second'].to_dict()\n",
    "first_dic = site_diction.set_index('key')['_first'].to_dict()\n",
    "second_dic['0'] = ''\n",
    "first_dic['0'] =''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### используем для создания признаков инструменты из text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = train_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                    'site6','site7', 'site8', 'site9', 'site10']].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_train.replace('0','')\n",
    "df_test = df_test.replace('0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_str = df_train.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_str = df_test.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "cvt = CountVectorizer( ngram_range = (1,2), min_df =2)\n",
    "tvt = TfidfVectorizer( ngram_range = (1,2), min_df =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим несколько таблиц признаков с разными значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngramm_analys (Train, Test, ngramm = (1,1) , min_freq = 1, analyzer = u'word'):\n",
    "    cvt = CountVectorizer( ngram_range = ngramm, min_df =min_freq, analyzer = analyzer)\n",
    "    tvt = TfidfVectorizer( ngram_range = ngramm, min_df =min_freq, analyzer = analyzer)\n",
    "    X_train_cvt = cvt.fit_transform(Train.values)\n",
    "    X_train_tvt = tvt.fit_transform(Train.values)\n",
    "    X_test_cvt  = cvt.transform(Test.values)\n",
    "    X_test_tvt  = tvt.transform(Test.values)\n",
    "    return X_train_cvt, X_train_tvt, X_test_cvt, X_test_tvt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим класификаторы по этим признакам** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ngramm =', (1, 1), '  min_fr =', 2, '  analys = word')\n",
      "fit...\n",
      "Wall time: 43.2 s\n",
      "Wall time: 43.1 s\n",
      "predict...\n",
      "cvt  (1, 1) 2 word\n",
      "('log', 0.28815218911735907)\n",
      "('svm', 0.26814939152328998)\n",
      "fit...\n",
      "Wall time: 45.6 s\n",
      "Wall time: 25.3 s\n",
      "predict...\n",
      "tvt  (1, 1) 2 word\n",
      "('log', 0.21307175828787242)\n",
      "('svm', 0.29724436984193592)\n",
      "('ngramm =', (1, 2), '  min_fr =', 2, '  analys = word')\n",
      "fit...\n",
      "Wall time: 55.9 s\n",
      "Wall time: 50.7 s\n",
      "predict...\n",
      "cvt  (1, 2) 2 word\n",
      "('log', 0.29353755770037765)\n",
      "('svm', 0.27885018883759966)\n",
      "fit...\n",
      "Wall time: 1min 6s\n",
      "Wall time: 51.5 s\n",
      "predict...\n",
      "tvt  (1, 2) 2 word\n",
      "('log', 0.19390823891453352)\n",
      "('svm', 0.30574206182682895)\n",
      "('ngramm =', (2, 2), '  min_fr =', 2, '  analys = word')\n",
      "fit...\n",
      "Wall time: 50.7 s\n",
      "Wall time: 35.4 s\n",
      "predict...\n",
      "cvt  (2, 2) 2 word\n",
      "('log', 0.22870331514897188)\n",
      "('svm', 0.24332074416002239)\n",
      "fit...\n",
      "Wall time: 42.5 s\n",
      "Wall time: 55.8 s\n",
      "predict...\n",
      "tvt  (2, 2) 2 word\n",
      "('log', 0.13480906420478389)\n",
      "('svm', 0.26335851167995522)\n",
      "('ngramm =', (2, 3), '  min_fr =', 2, '  analys = word')\n",
      "fit...\n",
      "Wall time: 1min 58s\n",
      "Wall time: 1min 24s\n",
      "predict...\n",
      "cvt  (2, 3) 2 word\n",
      "('log', 0.22835361589033432)\n",
      "('svm', 0.2356273604699958)\n",
      "fit...\n",
      "Wall time: 1min 57s\n",
      "Wall time: 1min 27s\n",
      "predict...\n",
      "tvt  (2, 3) 2 word\n",
      "('log', 0.11987690586095957)\n",
      "('svm', 0.26147013568331234)\n"
     ]
    }
   ],
   "source": [
    "dic_test ={}\n",
    "dic_cvt ={}\n",
    "dic_tvt ={}\n",
    "for ngramm, min_df in [((1,1),2) ,((1,2),2), ((2,2),2), ((2,3),2)]:\n",
    "    print ('ngramm =', ngramm, '  min_fr =', min_df,'  analys = word')\n",
    "    X_train_cvt, X_train_tvt, X_test_cvt, X_test_tvt =\\\n",
    "    ngramm_analys (df_train_str, df_test_str, ngramm  , min_df)\n",
    "            \n",
    "    key = str(ngramm) +\" \"+ str(min_df) +\" word\"\n",
    "    #dic_test[key] = (X_test_cvt, X_test_tvt)\n",
    "            \n",
    "    sgd_log_ngramm, sgd_svm_ngramm = fit_SGD(X_train_cvt, y, 'cvt  '+ key)\n",
    "    #dic_cvt[key] =(sgd_log_ngramm, sgd_svm_ngramm)\n",
    "            \n",
    "    sgd_log_ngramm, sgd_svm_ngramm = fit_SGD(X_train_tvt, y, 'tvt  '+ key)\n",
    "    #dic_tvt[key] =(sgd_log_ngramm, sgd_svm_ngramm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-выберем наиболее сильные признаки - ngramm =(1,2) min_df = 2 для log CountVectorizer() сделаем предсказание и будем использовать его в следующем классификаторе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_cvt, X_train_tvt, X_test_cvt, X_test_tvt =\\\n",
    "    ngramm_analys (df_train_str, df_test_str, (1,2) ,  min_freq = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 42.5 s\n",
      "Wall time: 31.5 s\n",
      "predict...\n",
      "cvt  ngramm=(1,2) min_df = 2\n",
      "('log', 0.29353755770037765)\n",
      "('svm', 0.27885018883759966)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_ngramm, sgd_svm_ngramm = fit_SGD(X_train_cvt, y, 'cvt  ngramm=(1,2) min_df = 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_cvt = sgd_log_ngramm.predict_proba(X_train_cvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new_add = hstack((X_train_tvt, csr_matrix(pred_cvt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 6min 37s\n",
      "Wall time: 4min 45s\n",
      "predict...\n",
      "tvt + model.cvt   ngramm=(1,2) min_df = 2\n",
      "('log', 0.25171352636732408)\n",
      "('svm', 0.3153937613652259)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_ngramm, sgd_svm_ngramm = fit_SGD(X_train_new_add, y, 'tvt + model.cvt   ngramm=(1,2) min_df = 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- добавим предсказание временной модели к данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new_add = hstack((X_train_tvt, csr_matrix(pred_cvt), csr_matrix(logit_valid_pred_T))) #предсказание временной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим в качестве признаков первое и второе доменное имя**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = train_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                    'site6','site7', 'site8', 'site9', 'site10']].fillna(0).astype(int).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype(int).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>net</td>\n",
       "      <td>com</td>\n",
       "      <td>net</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>io</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "      <td>org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           site1 site2 site3 site4 site5 site6 site7 site8 site9 site10\n",
       "session_id                                                             \n",
       "1            com   com   com    fr   com   com   net   com   net    com\n",
       "2            com   com   com   com   com   com   com   com   com    com\n",
       "3            com    fr    io   com   com    fr    fr   com   com     fr\n",
       "4            com   com   com   com   com   com   com   com   com    com\n",
       "5            org   org   org   org   org    fr    fr   org   org    org"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_train.applymap(lambda x: second_dic[x])\n",
    "df3 = df_train.applymap(lambda x: first_dic[x])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_test = df_test.applymap(lambda x: second_dic[x])\n",
    "df3_test = df_test.applymap(lambda x: first_dic[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train2 = df_train2.apply(lambda x: ' '.join(x) if x != '0' else '', axis =1)\n",
    "df_test2 = df_test2.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.apply(lambda x: ' '.join(x), axis =1)\n",
    "df2_test = df2_test.apply(lambda x: ' '.join(x), axis =1)\n",
    "df3 = df3.apply(lambda x: ' '.join(x), axis =1)\n",
    "df3_test = df3_test.apply(lambda x: ' '.join(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "cvt = CountVectorizer( ngram_range = (1,2), min_df =2)#, analyzer=u'char')\n",
    "tvt = TfidfVectorizer( ngram_range = (1,2), min_df =2)#, analyzer=u'char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%time X_train_ngram_c = cvt.fit_transform(df_train2.values)\n",
    "X_train_ngram_t = tvt.fit_transform(df_train2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%time X_test_ngram_c = cvt.transform(df_test2.values)\n",
    "X_test_ngram_t = tvt.transform(df_test2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 60252), (95319, 60252))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c.shape, X_train_ngram_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvt = CountVectorizer()\n",
    "tvt = TfidfVectorizer()\n",
    "X_train_ngram_c2 = cvt.fit_transform(df2.values)\n",
    "X_train_ngram_t2 = tvt.fit_transform(df2.values)\n",
    "X_test_ngram_c2 = cvt.transform(df2_test.values)\n",
    "X_test_ngram_t2 = tvt.transform(df2_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 9710), (95319, 9710))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c2.shape, X_train_ngram_t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cvt = CountVectorizer()#, analyzer=u'char')\n",
    "tvt = TfidfVectorizer()#, analyzer=u'char')\n",
    "X_train_ngram_c3 = cvt.fit_transform(df3.values)\n",
    "X_train_ngram_t3 = tvt.fit_transform(df3.values)\n",
    "X_test_ngram_c3 = cvt.transform(df3_test.values)\n",
    "X_test_ngram_t3 = tvt.transform(df3_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 144), (95319, 144))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram_c3.shape, X_train_ngram_t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 70106), (95319, 70106))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new_c = hstack((X_train_ngram_c, X_train_ngram_c2, X_train_ngram_c3))\n",
    "X_train_new_t = hstack((X_train_ngram_t, X_train_ngram_t2, X_train_ngram_t3))\n",
    "X_train_new_c.shape, X_train_new_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 55.8 s\n",
      "Wall time: 37.1 s\n",
      "predict...\n",
      "cvt ngramm + dominate names\n",
      "('log', 0.2470625262274444)\n",
      "('svm', 0.23080151070079732)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new_c, sgd_svm_new_c = fit_SGD(X_train_new_c, y, 'cvt ngramm + dominate names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 46.7 s\n",
      "Wall time: 35.3 s\n",
      "predict...\n",
      "tvt ngramm + dominate names\n",
      "('log', 0.23569730032172331)\n",
      "('svm', 0.30661630997342287)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new_t, sgd_svm_new_t = fit_SGD(X_train_new_t, y, 'tvt ngramm + dominate names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 70106), (95319, 70106))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_new2_c = hstack((X_train_ngram_c, X_train_ngram_c2))\n",
    "X_train_new2_t = hstack((X_train_ngram_t, X_train_ngram_t2))\n",
    "X_train_new_c.shape, X_train_new_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 50.2 s\n",
      "Wall time: 37.3 s\n",
      "predict...\n",
      "cvt ngramm + 1 dominate names\n",
      "('log', 0.26409288012309412)\n",
      "('svm', 0.24422996223248006)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new2_c, sgd_svm_new2_c = fit_SGD(X_train_new2_c, y, 'cvt ngramm + 1 dominate names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 58 s\n",
      "Wall time: 43.3 s\n",
      "predict...\n",
      "tvt ngramm + 1 dominate names\n",
      "('log', 0.24674779689467058)\n",
      "('svm', 0.31095258078052873)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_new2_t, sgd_svm_new2_t = fit_SGD(X_train_new2_t, y, 'tvt ngramm + 1 dominate names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**добавим временные признаки к ngramm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 17), (136496, 7))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_hour_encod.shape, day_of_week_encod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_hour_train = start_hour_encod.tocsr()[: 95319, :]\n",
    "start_hour_test = start_hour_encod.tocsr()[95319 :, :]\n",
    "day_of_week_train = day_of_week_encod.tocsr()[: 95319, :]\n",
    "day_of_week_test = day_of_week_encod.tocsr()[95319 :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 60276), (95319, 60276))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_time_c = hstack((X_train_ngram_c, start_hour_train, day_of_week_train))\n",
    "X_train_time_t = hstack((X_train_ngram_t, start_hour_train, day_of_week_train))\n",
    "X_train_time_c.shape, X_train_time_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 41.3 s\n",
      "Wall time: 32.8 s\n",
      "predict...\n",
      "cvt ngramm + time\n",
      "('log', 0.35798713106728214)\n",
      "('svm', 0.33676038606798153)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time, sgd_svm_time = fit_SGD(X_train_time_c, y, 'cvt ngramm + time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим те же класификаторы что и до этого с преобразованием TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 46.3 s\n",
      "Wall time: 35.5 s\n",
      "predict...\n",
      "tvt ngramm + time\n",
      "('log', 0.25884739124353057)\n",
      "('svm', 0.36071478528465517)\n"
     ]
    }
   ],
   "source": [
    "sgd_log_time2, sgd_svm_time2 = fit_SGD(X_train_time_t, y, 'tvt ngramm + time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new = hstack((X_train_sparse, X_train_ngram))\n",
    "X_test_new = hstack((X_test_sparse, X_test_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {'penalty': ('l2', 'l1'), \n",
    "              'alpha': [0.00015, 0.00012, 0.00008, 0.0001]\n",
    "              }           \n",
    "parameters = {'alpha': [0.00007, 0.00006, 0.00009, 0.00012, 0.00008, 0.0001]\n",
    "              }           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(sgd_svm, parameters, cv =skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=7, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=7, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ('l2', 'l1'), 'alpha': [0.00015, 0.00012, 8e-05, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2875919847728669, {'alpha': 8e-05, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30196530983354314"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, clf.best_estimator_ .predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28089264571437134, {'alpha': 0.0001, 'penalty': 'l2'})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите в файл *answer5_2.txt* через пробел доли правильных ответов логистической регресии и линейного SVM, обученных с помощью стохастического градиентного спуска, на отложенной выборке. Округлите до 3 знаков после разделителя. Полученный файл будет ответом на 2 вопрос теста.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer_to_file('{}  {}'.format(round(accuracy_score(y_valid, logit_valid_pred), 3),\n",
    "                                    round(accuracy_score(y_valid, svm_valid_pred), 3)),\n",
    "                     'answer5_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз для тестовой выборки с помощью sgd_logit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = sgd_logit.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = clf.best_estimator_ .predict(X_test_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Далее будет короткое Peer-Review задание, в котором надо проверить посылки друг друга на Kaggle. Поэтому дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] Coursera Username\", чтоб можно было идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/leaderboard).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить, дополнительные баллы будут для тех, кто попадет в топ-10 и топ-30 по итогам этой недели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_submission_file (logit_test_pred, 'kaggle_data/[YDF&MIPT]_Coursera_Oleg67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в финальный проект (.pdf или .ipynb).\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных 550 пользователями веб-страницах (550 csv-файлов в *train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем тьюториал по Vowpal Wabbit и попробуем его в деле – в соревновании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1 = train_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clu = DBSCAN(eps=0.6, min_samples=500 )\n",
    "%time clus = clu.fit_predict(matrix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(clus, return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\mixture\\base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianGaussianMixture(covariance_prior=None, covariance_type='full',\n",
       "            degrees_of_freedom_prior=None, init_params='kmeans',\n",
       "            max_iter=100, mean_precision_prior=None, mean_prior=None,\n",
       "            n_components=20, n_init=1, random_state=None, reg_covar=1e-06,\n",
       "            tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "            weight_concentration_prior=None,\n",
       "            weight_concentration_prior_type='dirichlet_process')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "BGM = BayesianGaussianMixture(n_components =20)\n",
    "%time BGM.fit(df_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lables = BGM.predict(df_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19], dtype=int64),\n",
       " array([12802,  3332,  3239,  8405,  2428,  3098,  2490,  5426,  2067,\n",
       "         3414,  2800,  6209, 16916,  2866,  5041,  2243,  2183,  2292,\n",
       "         5109,  2959], dtype=int64))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(lables, return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
