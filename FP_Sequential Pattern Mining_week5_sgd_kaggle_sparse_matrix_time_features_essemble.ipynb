{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Identify Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/data) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle_data/train_sessions.csv', index_col='session_id')\n",
    "test_df = pd.read_csv('kaggle_data/test_sessions.csv', index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>932.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>151.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3191.0</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668</td>\n",
       "      <td>2014-02-14 15:16:45</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:17:13</td>\n",
       "      <td>598.0</td>\n",
       "      <td>2014-02-14 15:20:47</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2014-02-14 15:21:13</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>2014-02-14 15:21:14</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-02-14 15:21:15</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1943</td>\n",
       "      <td>2014-03-17 15:19:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:20:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:21:40</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:10</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-17 15:22:39</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:41</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:42</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>2014-03-17 15:22:43</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2   site3  \\\n",
       "session_id                                                                    \n",
       "1               8  2014-01-04 08:44:50    11.0  2014-01-04 08:44:50    82.0   \n",
       "2             111  2014-03-18 10:33:20    78.0  2014-03-18 10:33:31   151.0   \n",
       "3              11  2014-12-02 13:13:41  3187.0  2014-12-02 13:13:41   132.0   \n",
       "4             668  2014-02-14 15:16:45  1965.0  2014-02-14 15:17:13   598.0   \n",
       "5            1943  2014-03-17 15:19:40  1943.0  2014-03-17 15:20:10  1943.0   \n",
       "\n",
       "                          time3   site4                time4   site5  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:19    68.0  2014-01-04 08:45:25     8.0   \n",
       "2           2014-03-18 10:33:31   111.0  2014-03-18 10:33:31  1401.0   \n",
       "3           2014-12-02 13:13:42   496.0  2014-12-02 13:13:42  1969.0   \n",
       "4           2014-02-14 15:20:47  1965.0  2014-02-14 15:21:13   284.0   \n",
       "5           2014-03-17 15:21:40  1943.0  2014-03-17 15:22:10  1943.0   \n",
       "\n",
       "                          time5   ...                  time6   site7  \\\n",
       "session_id                        ...                                  \n",
       "1           2014-01-04 08:45:25   ...    2014-01-04 08:45:51  8403.0   \n",
       "2           2014-03-18 10:33:31   ...    2014-03-18 10:33:32  1375.0   \n",
       "3           2014-12-02 13:13:45   ...    2014-12-02 13:13:45  3187.0   \n",
       "4           2014-02-14 15:21:14   ...    2014-02-14 15:21:14    38.0   \n",
       "5           2014-03-17 15:22:39   ...    2014-03-17 15:22:39  1952.0   \n",
       "\n",
       "                          time7   site8                time8   site9  \\\n",
       "session_id                                                             \n",
       "1           2014-01-04 08:45:51   932.0  2014-01-04 08:45:53  3260.0   \n",
       "2           2014-03-18 10:33:32    38.0  2014-03-18 10:33:32  1401.0   \n",
       "3           2014-12-02 13:13:45    82.0  2014-12-02 13:13:46  3191.0   \n",
       "4           2014-02-14 15:21:14  4451.0  2014-02-14 15:21:14  4537.0   \n",
       "5           2014-03-17 15:22:41  1943.0  2014-03-17 15:22:41  1943.0   \n",
       "\n",
       "                          time9  site10               time10 user_id  \n",
       "session_id                                                            \n",
       "1           2014-01-04 08:45:53     8.0  2014-01-04 08:45:53    1845  \n",
       "2           2014-03-18 10:33:32    97.0  2014-03-18 10:33:34    3322  \n",
       "3           2014-12-02 13:13:46  3184.0  2014-12-02 13:13:47    2003  \n",
       "4           2014-02-14 15:21:15    11.0  2014-02-14 15:21:15    1373  \n",
       "5           2014-03-17 15:22:42  1943.0  2014-03-17 15:22:43    1737  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 21), (41177, 20), (136496, 21))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 550 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза ID пользователя будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sites = train_df[['site1', 'site2', 'site3','site4','site5','site6','site7',\n",
    "          'site8', 'site9', 'site10', 'user_id']].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ures_top_sites(data, threshold=1):\n",
    "    \"\"\" input dataframe\n",
    "    return \n",
    "    list of top sites for users with the threshold\n",
    "    dictionary user, top site's list \"\"\"\n",
    "    top_users_sites =[]\n",
    "    dic_user_top ={}\n",
    "    \n",
    "    for user, values in pd.groupby(data, by = 'user_id'):\n",
    "        n,m = values.shape\n",
    "        sites, freq = np.unique(np.ravel(values.values), return_counts=True)\n",
    "        mask = np.logical_not(np.logical_not(sites))\n",
    "        sites = sites[mask]\n",
    "        freq = freq[mask]\n",
    "        sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "        #user_site_top = sort_sites[:threshold]\n",
    "        top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "        dic_user_top[user] = top_list\n",
    "        top_users_sites.append(top_list)\n",
    "    return np.unique(top_users_sites), dic_user_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tops_sites(data, threshold=3):\n",
    "    \"\"\" input dataframe\n",
    "    return top sites with the threshold\"\"\"\n",
    "    data_ravel =np.ravel(data.drop(['user_id'], axis =1).values)\n",
    "    sites, freq = np.unique(data_ravel, return_counts=True)\n",
    "    mask = np.logical_not(np.logical_not(sites))\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    mask= freq >= threshold\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "    #user_site_top = sort_sites[:threshold]\n",
    "    top_list = np.array([x[0] for x in  sort_sites])\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_sites(data, threshold=50):\n",
    "    \"\"\" input dataframe\n",
    "    return top sites with the threshold\"\"\"\n",
    "    data_ravel =np.ravel(data.drop(['user_id'], axis =1).values)\n",
    "    sites, freq = np.unique(data_ravel, return_counts=True)\n",
    "    mask = np.logical_not(np.logical_not(sites))\n",
    "    sites = sites[mask]\n",
    "    freq = freq[mask]\n",
    "    sort_sites = sorted([(s, fr) for s,fr in zip(sites, freq)], key= lambda x: x[1], reverse =True )\n",
    "    #user_site_top = sort_sites[:threshold]\n",
    "    top_list = np.array([x[0] for x in  sort_sites[:threshold]])\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из 50 самых популярных сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_sites = top_sites(train_sites, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- создадим список из сайтов которые повторяются минимум 2 раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sites = tops_sites(train_sites, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13988"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in2D(data, ar2):\n",
    "    \"\"\" return the mask like data where True only the element of data is in ar2\"\"\"\n",
    "    data = data.values\n",
    "    n,m = data.shape\n",
    "    mask = np.in1d(np.ravel(data),ar2)\n",
    "    mask_2d = mask.reshape((n,m))\n",
    "    return mask_2d\n",
    "#np.where(mask_2d,train_sites.head(3).values, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создадим признаки связанные с временем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>time4</th>\n",
       "      <th>time5</th>\n",
       "      <th>time6</th>\n",
       "      <th>time7</th>\n",
       "      <th>time8</th>\n",
       "      <th>time9</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:44:50</td>\n",
       "      <td>2014-01-04 08:45:19</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:25</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:51</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "      <td>2014-01-04 08:45:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-18 10:33:20</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:31</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:32</td>\n",
       "      <td>2014-03-18 10:33:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:41</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:42</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:45</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:46</td>\n",
       "      <td>2014-12-02 13:13:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time1                time2                time3  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:44:50  2014-01-04 08:44:50  2014-01-04 08:45:19   \n",
       "2           2014-03-18 10:33:20  2014-03-18 10:33:31  2014-03-18 10:33:31   \n",
       "3           2014-12-02 13:13:41  2014-12-02 13:13:41  2014-12-02 13:13:42   \n",
       "\n",
       "                          time4                time5                time6  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:25  2014-01-04 08:45:25  2014-01-04 08:45:51   \n",
       "2           2014-03-18 10:33:31  2014-03-18 10:33:31  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:42  2014-12-02 13:13:45  2014-12-02 13:13:45   \n",
       "\n",
       "                          time7                time8                time9  \\\n",
       "session_id                                                                  \n",
       "1           2014-01-04 08:45:51  2014-01-04 08:45:53  2014-01-04 08:45:53   \n",
       "2           2014-03-18 10:33:32  2014-03-18 10:33:32  2014-03-18 10:33:32   \n",
       "3           2014-12-02 13:13:45  2014-12-02 13:13:46  2014-12-02 13:13:46   \n",
       "\n",
       "                         time10  \n",
       "session_id                       \n",
       "1           2014-01-04 08:45:53  \n",
       "2           2014-03-18 10:33:34  \n",
       "3           2014-12-02 13:13:47  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_time = train_test_df[['time1', 'time2', 'time3', 'time4','time5', 'time6',\n",
    "                        'time7','time8', 'time9', 'time10']].fillna(np.datetime64('nat'))\n",
    "train_test_time.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site1', 'site2', 'site3', 'site4','site5', \n",
    "                                     'site6','site7', 'site8', 'site9', 'site10']].fillna(0).astype('int')\n",
    "y = train_df['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask1 = in2D(train_test_df_sites, top_sites) # маска для сайтов самых популярных у пользователей\n",
    "mask2 = in2D(train_test_df_sites, list_sites)  # маска для сайтов которые встречаются в выборке больше 2 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**выделим сайты которые идут за сымыми популярными**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только сайты с частотой более 2\n",
    "train_test_df_sites = np.where(mask2, train_test_df_sites.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_sites_shift = train_test_df_sites[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496L, 9L), (136496L, 9L))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_top_site = np.where(mask1[:,:-1], train_test_df_sites_shift, 0) # site after top50\n",
    "next_top_site.shape,  train_test_df_sites_shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_time.apply(pd.to_datetime).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# оставим только сайты с частотой более 2\n",
    "train_test_df_time = np.where(mask2, train_test_df_time, train_test_df_time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# начало сессии по дням недели\n",
    "day_of_week = (pd.to_datetime(np.min(train_test_df_time, axis =1))).dayofweek\n",
    "day_of_week = day_of_week.reshape(train_test_df_time.shape[0],1).astype(int) \n",
    "# время начала сессии в часах\n",
    "start_hour = (pd.to_datetime(np.min(train_test_df_time, axis =1))).hour\n",
    "start_hour = start_hour.reshape(train_test_df_time.shape[0],1).astype(int)\n",
    "# время захода на сайт из top50  \n",
    "time_tops_site = np.where(mask1, train_test_df_time, train_test_df_time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_df_time = np.where(mask2, train_test_df_time, np.datetime64('nat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время перехода между сайтами в 3 сек \n",
    "train_test_time_diff = (np.diff(train_test_df_time, axis =1)/np.timedelta64(2, 's')).round()\n",
    "#test_time_diff = (np.diff(test_time, axis =1)/np.timedelta64(3, 's')).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# время перехода между сайтами из top50 and next в 2 сек \n",
    "next_time_diff_tops = np.where(mask1[:,:-1], train_test_time_diff, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_time_diff_tops.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время захода на сайт из top50 в часах\n",
    "top_start_hour = (pd.to_datetime(np.min(time_tops_site, axis =1))).hour \n",
    "top_start_hour = top_start_hour.reshape(time_tops_site.shape[0],1).astype(int) +1\n",
    "top_start_hour = np.nan_to_num(top_start_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**перевод категориальных признаков в двоичные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136496, 7), (136496, 17), (136496, 17))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "day_of_week_encode = OneHotEncoder().fit_transform(day_of_week) # перевод категориальных признаков в двоичные\n",
    "start_hour_encode = OneHotEncoder().fit_transform(start_hour)\n",
    "top_start_hour_encode = OneHotEncoder().fit_transform(top_start_hour)\n",
    "day_of_week_encode.shape, start_hour_encode.shape, top_start_hour_encode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу train_test_df_sites – потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_sparse_matrix (matrix):\n",
    "    \"\"\"переводим обычную матрицу в разреженноу матрицу \n",
    "    где \n",
    "    номер столбца это уникальное число из исходной матрицы от 1  до максимального\n",
    "    значение в строке это сколько раз уникальное число встречалось в строке оригинальной матрицы\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "        \n",
    "    NMZ = np.prod(np.array(matrix.shape)) # колличество элементов в matrix\n",
    "    data = np.array([1]*NMZ)\n",
    "    indptr = np.arange(0, NMZ+matrix.shape[1], matrix.shape[1])\n",
    "    return csr_matrix((data, matrix.reshape(-1), indptr), dtype=int)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 21532)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_test_sparse = matrix_to_sparse_matrix(train_test_df_sites.astype(np.int64))\n",
    "Train_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 21532)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Next_top_site_sparse = matrix_to_sparse_matrix(next_top_site.astype(np.int64))\n",
    "Next_top_site_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 900)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Next_time_tops_sparse =  matrix_to_sparse_matrix(next_time_diff_tops)\n",
    "Next_time_tops_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**объединим все признаки сайты время начала сесии время захода на сайт из top200 and top5user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 21532, 136496, 21573)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour\n",
    "Train_Test= hstack((Train_test_sparse, day_of_week_encode, start_hour_encode, top_start_hour_encode ))\n",
    "n1 ,m1 = Train_test_sparse.shape \n",
    "n2 ,m2 = Train_Test.shape\n",
    "n1, m1, n2, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 43105)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Test = hstack((Train_Test,Next_top_site_sparse))\n",
    "n3, m3 =Train_Test.shape\n",
    "n3, m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 44005)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Test = hstack((Train_Test, Next_time_tops_sparse))\n",
    "n4, m4 =Train_Test.shape\n",
    "n4, m4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**разделим признаки на Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train1 = X_train_spare.tocsc()[:,:m1]   # only sites\n",
    "X_test1  = X_test_sparse.tocsc()[:,:m1] \n",
    "X_train2 = X_train_spare.tocsc()[:,m1:m2] # day_of_week+ start_hour + top50 hour\n",
    "X_test2  = X_test_sparse.tocsc()[:,m1:m2]\n",
    "X_train3 = X_train_spare.tocsc()[:,m2:m3] #  next after top50 site\n",
    "X_test3  = X_test_sparse.tocsc()[:,m2:m3]\n",
    "X_train4 = X_train_spare.tocsc()[:,m3:]   # time of next top50 site\n",
    "X_test4  = X_test_sparse.tocsc()[:,m3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создайте объекты sklearn.linear_model.SGDClassifier с логистической функцией потерь и с hinge loss (логистическая регрессия и линейный SVM соответственно) . Остальные параметры  по умолчанию, разве что alpha=0.00007, n_jobs=-1 никогда не помешает. Обучите  модели на выборке (X_train, y_train).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**обучим и проверим на валидации два классификатора SGDClassifier( alpha=0.00007, loss ='log') и SGDClassifier( alpha=0.00007, loss ='hinge')**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**будем последовательно обучать алгрритмы и добовлять к их Ответам новые признаки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, y, test_size=0.2, \n",
    "                                                     random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_spet_fit (X_train, y_train, X_valid, y_valid, explain = \"fit_SGD\"):\n",
    "    # Разобьем обучающую выборку на 2 части в пропорции 7/3\n",
    "    \n",
    "    sgd_logit = SGDClassifier( alpha=0.00007, loss ='log', random_state = 0,  n_jobs = -1)\n",
    "    sgd_svm  = SGDClassifier( alpha=0.00007, loss ='hinge', random_state=0,  n_jobs = -1)\n",
    "    print('fit...')\n",
    "    %time sgd_logit.fit(X_train, y_train)\n",
    "    %time sgd_svm.fit(X_train, y_train)\n",
    "    # Сделаем прогнозы на отложенной выборке (X_valid, y_valid)\n",
    "    print('predict...')\n",
    "    pred_log = sgd_logit.predict(X_valid)\n",
    "    pred_svm = sgd_svm.predict(X_valid)\n",
    "    print (explain)\n",
    "    print('log',accuracy_score(y_valid, pred_log))\n",
    "    print('svm',accuracy_score(y_valid, pred_svm))\n",
    "    return sgd_logit, sgd_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 36.1 s\n",
      "Wall time: 26.1 s\n",
      "predict...\n",
      "sites only\n",
      "('log', 0.28960344104070501)\n",
      "('svm', 0.26851657574485943)\n"
     ]
    }
   ],
   "source": [
    "log_pred1, svm_pred1 = one_spet_fit (X_train, y_train, X_valid, y_valid, \\\n",
    "                                     explain = \"sites only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_step (X_train, X_test, clf1, clf2):\n",
    "    \n",
    "    n1,_ = X_train.shape\n",
    "    tr_pred1= clf1.predict(X_train).reshape(n1,1)\n",
    "    tr_pred2= clf2.predict(X_train).reshape(n1,1)\n",
    "    \n",
    "    n2,_ = X_test.shape\n",
    "    ts_pred1= clf1.predict(X_test).reshape(n2,1)\n",
    "    ts_pred2= clf2.predict(X_test).reshape(n2,1)\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    OHEn = OneHotEncoder()\n",
    "    tr_encode1 = OHEn.fit_transform(tr_pred1)\n",
    "    ts_encode1 = OHEn.transform(ts_pred1)\n",
    "    tr_encode2 = OHEn.fit_transform(tr_pred2)\n",
    "    ts_encode2 = OHEn.transform(ts_pred2)\n",
    "    from scipy.sparse import hstack\n",
    "    \n",
    "    return hstack((tr_encode1, tr_encode2)), hstack((ts_encode1, ts_encode2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1079), (41177, 1079))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_step1, X_test_step1 = next_step (X_train1, X_test1, log_pred1, svm_pred1)\n",
    "X_train_step1.shape, X_test_step1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1120), (41177, 1120))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# step1 + day_of_week+ start_hour + top50 hour \n",
    "X_train_2 = hstack((X_train2, X_train_step1))\n",
    "X_test_2  = hstack((X_test2,  X_test_step1))\n",
    "X_train_2.shape, X_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_2, y, test_size=0.2, \n",
    "                                                     random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 34 s\n",
      "Wall time: 23.1 s\n",
      "predict...\n",
      "sites only\n",
      "('log', 0.33419009651699538)\n",
      "('svm', 0.32836760386067981)\n"
     ]
    }
   ],
   "source": [
    "log_pred2, svm_pred2 = one_spet_fit (X_train, y_train, X_valid, y_valid, \\\n",
    "                                     explain = \"step1 + time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1036), (41177, 1036))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_step2, X_test_step2 = next_step (X_train_2, X_test_2, log_pred2, svm_pred2)\n",
    "X_train_step2.shape, X_test_step2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 22568), (41177, 22568))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# step1 + day_of_week+ start_hour + top50 hour \n",
    "X_train_3 = hstack((X_train3, X_train_step2))\n",
    "X_test_3  = hstack((X_test3,  X_test_step2))\n",
    "X_train_3.shape, X_test_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_3, y, test_size=0.2, \n",
    "                                                     random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 35.8 s\n",
      "Wall time: 26.3 s\n",
      "predict...\n",
      "sites only\n",
      "('log', 0.34232060428031891)\n",
      "('svm', 0.33356063785144774)\n"
     ]
    }
   ],
   "source": [
    "log_pred3, svm_pred3 = one_spet_fit (X_train, y_train, X_valid, y_valid, \\\n",
    "                                     explain = \"step2 + next sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1066), (41177, 1066))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_step3, X_test_step3 = next_step (X_train_3, X_test_3, log_pred3, svm_pred3)\n",
    "X_train_step3.shape, X_test_step3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95319, 1966), (41177, 1966))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# step1 + day_of_week+ start_hour + top50 hour \n",
    "X_train_4 = hstack((X_train4, X_train_step3))\n",
    "X_test_4  = hstack((X_test4,  X_test_step3))\n",
    "X_train_4.shape, X_test_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_4, y, test_size=0.2, \n",
    "                                                     random_state=7, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 31.1 s\n",
      "Wall time: 23 s\n",
      "predict...\n",
      "step2 + next sites\n",
      "('log', 0.34420898027696184)\n",
      "('svm', 0.3391733109525808)\n"
     ]
    }
   ],
   "source": [
    "log_pred4, svm_pred4 = one_spet_fit (X_train, y_train, X_valid, y_valid, \\\n",
    "                                     explain = \"step3 + time next sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_2.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_2.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 41.3 s\n",
      "Wall time: 31.3 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour + next after top50 site\n",
      "('log', 0.35763743180864455)\n",
      "('svm', 0.33004616030214018)\n"
     ]
    }
   ],
   "source": [
    "log2, svm2 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour + next after top50 site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 22473)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour + \n",
    "Train_Test_3 = hstack((Train_Test_1, Next_time_tops_sparse))\n",
    "Train_Test_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_3.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_3.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 43.2 s\n",
      "Wall time: 33.2 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour + time of next top50 site\n",
      "('log', 0.36151909357952161)\n",
      "('svm', 0.33648062666107148)\n"
     ]
    }
   ],
   "source": [
    "log3, svm3 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour + time of next top50 site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136496, 44005)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "# sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\n",
    "Train_Test_4 = hstack((Train_Test_2, Next_time_tops_sparse))\n",
    "Train_Test_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_spare = Train_Test_4.tocsr()[: 95319, :]\n",
    "X_test_sparse = Train_Test_4.tocsr()[95319 :, :]\n",
    "y = train_df['user_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "Wall time: 43.8 s\n",
      "Wall time: 32.9 s\n",
      "predict...\n",
      "sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\n",
      "('log', 0.35616869492236675)\n",
      "('svm', 0.32955658134004756)\n"
     ]
    }
   ],
   "source": [
    "log4, svm4 = fit_SGD (X_train_spare, y,\\\n",
    "             explain = \"sites + day_of_week+ start_hour + top50 hour +next after top50 site+  time of next top50 site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогноз для тестовой выборки с помощью log3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test = log3.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Далее будет короткое Peer-Review задание, в котором надо проверить посылки друг друга на Kaggle. Поэтому дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] Coursera Username\", чтоб можно было идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/identify-me-if-you-can-yandex-mipt/leaderboard).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить, дополнительные баллы будут для тех, кто попадет в топ-10 и топ-30 по итогам этой недели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_submission_file (pred_test, 'kaggle_data/[YDF&MIPT]_Coursera_Oleg67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в финальный проект (.pdf или .ipynb).\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных 550 пользователями веб-страницах (550 csv-файлов в *train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем тьюториал по Vowpal Wabbit и попробуем его в деле – в соревновании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
